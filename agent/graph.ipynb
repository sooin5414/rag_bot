{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ad3dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/edutem/.cache/pypoetry/virtualenvs/rag-bot-vbdTYmCJ-py3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community.retrievers.contextual_compression'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_huggingface\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbeddings\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvectorstores\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_community\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcontextual_compression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ContextualCompressionRetriever\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_classic\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_compressors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LLMChainExtractor\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrapidfuzz\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m process, fuzz\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'langchain_community.retrievers.contextual_compression'"
     ]
    }
   ],
   "source": [
    "# agent/graph.py\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import unicodedata\n",
    "from typing import TypedDict, List, Optional, Literal, Dict, Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "from langchain_classic.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 0. GraphState ì •ì˜ (state.pyì— ë³„ë„ ë¶„ë¦¬í•´ë„ ë¨)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "class GraphState(TypedDict, total=False):\n",
    "    messages: List[BaseMessage]\n",
    "    question: str\n",
    "    kg_result: Optional[Dict[str, Any]]\n",
    "    rag_docs: Optional[List[Any]]\n",
    "    answer: Optional[str]\n",
    "    route: Optional[Literal[\"kg\", \"rag\"]]\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Env / ì „ì—­ ë¦¬ì†ŒìŠ¤ ì´ˆê¸°í™” (ë²¡í„°ìŠ¤í† ì–´, KG, LLM, retriever, prompt)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ğŸ”¹ ë²¡í„°ìŠ¤í† ì–´\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"intfloat/multilingual-e5-large\")\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=\"english_grammar_chunked\",\n",
    ")\n",
    "\n",
    "# ğŸ”¹ Knowledge Graph\n",
    "with open(\"/data/edutem/sooine/rag_bot/knowledge_graph.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    KNOWLEDGE_GRAPH = json.load(f)\n",
    "\n",
    "# ğŸ”¹ LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# ğŸ”¹ Prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\\n\\n<context>\\n{context}\\n</context>\",\n",
    "        ),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ğŸ”¹ ê³ ê¸‰ Retriever (Contextual Compression)\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 15})\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_retriever=base_retriever,\n",
    "    base_compressor=compressor,\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 2. KG ìœ í‹¸ (ë„¤ ì½”ë“œ ê·¸ëŒ€ë¡œ ì˜®ê¹€)\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def normalize(s: str):\n",
    "    return unicodedata.normalize(\"NFC\", s.lower().replace(\" \", \"\"))\n",
    "\n",
    "\n",
    "def fuzzy_match_topic(query, topic_list):\n",
    "    q = normalize(query)\n",
    "    candidates = [normalize(t) for t in topic_list]\n",
    "    match, score, idx = process.extractOne(q, candidates, scorer=fuzz.ratio)\n",
    "    return topic_list[idx] if score > 70 else None\n",
    "\n",
    "\n",
    "def search_in_knowledge_graph(query: str) -> Optional[Dict[str, Any]]:\n",
    "    q = query.lower().strip()\n",
    "    topic_list = list(KNOWLEDGE_GRAPH.keys())\n",
    "\n",
    "    best = fuzzy_match_topic(query, topic_list)\n",
    "    if best:\n",
    "        return {\n",
    "            \"type\": \"main_topic\",\n",
    "            \"main_topic\": best,\n",
    "            \"data\": KNOWLEDGE_GRAPH[best],\n",
    "        }\n",
    "\n",
    "    for topic in topic_list:\n",
    "        if topic.lower() == q:\n",
    "            return {\n",
    "                \"type\": \"main_topic\",\n",
    "                \"main_topic\": topic,\n",
    "                \"data\": KNOWLEDGE_GRAPH[topic],\n",
    "            }\n",
    "\n",
    "    for topic in topic_list:\n",
    "        if q in topic.lower():\n",
    "            return {\n",
    "                \"type\": \"main_topic\",\n",
    "                \"main_topic\": topic,\n",
    "                \"data\": KNOWLEDGE_GRAPH[topic],\n",
    "            }\n",
    "\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "    for main_topic, topic_data in KNOWLEDGE_GRAPH.items():\n",
    "        for sub_id, sub_data in topic_data[\"sub_topics\"].items():\n",
    "            score = 0\n",
    "            if q in sub_data[\"title\"].lower():\n",
    "                score += 3\n",
    "            if q in sub_data[\"concept\"].lower():\n",
    "                score += 2\n",
    "            for ex in sub_data.get(\"examples\", []):\n",
    "                if q in ex.lower():\n",
    "                    score += 1\n",
    "                    break\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = {\n",
    "                    \"type\": \"sub_topic\",\n",
    "                    \"main_topic\": main_topic,\n",
    "                    \"sub_topic_id\": sub_id,\n",
    "                    \"data\": sub_data,\n",
    "                }\n",
    "\n",
    "    if best_match and best_score >= 1:\n",
    "        return best_match\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "def format_kg_result_for_answer(kg_result: Dict[str, Any]) -> str:\n",
    "    \"\"\"Streamlitì—ì„œ í•˜ë˜ ì¶œë ¥ íŒ¨í„´ì„ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°.\"\"\"\n",
    "    data = kg_result[\"data\"]\n",
    "    concept = data.get(\"concept\", \"\")\n",
    "    examples = data.get(\"examples\", [])\n",
    "\n",
    "    lines = [concept]\n",
    "    if examples:\n",
    "        lines.append(\"\\nì˜ˆë¬¸:\")\n",
    "        for ex in examples:\n",
    "            lines.append(f\"- {ex}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 3. RAG ìœ í‹¸\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(getattr(doc, \"page_content\", \"\") for doc in docs)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 4. Nodes\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def prepare_question_node(state: GraphState) -> Dict:\n",
    "    \"\"\"messagesì—ì„œ ë§ˆì§€ë§‰ HumanMessageë¥¼ questionìœ¼ë¡œ ì •ê·œí™”.\"\"\"\n",
    "    if state.get(\"question\"):\n",
    "        return {}\n",
    "    msgs = state.get(\"messages\", [])\n",
    "    q = \"\"\n",
    "    for m in reversed(msgs):\n",
    "        if isinstance(m, HumanMessage):\n",
    "            q = m.content\n",
    "            break\n",
    "    return {\"question\": q}\n",
    "\n",
    "\n",
    "def search_kg_node(state: GraphState) -> Dict:\n",
    "    \"\"\"KGì—ì„œ ë¨¼ì € ì°¾ê³ , ìˆìœ¼ë©´ route='kg', ì—†ìœ¼ë©´ route='rag'.\"\"\"\n",
    "    q = state.get(\"question\", \"\")\n",
    "    kg_result = search_in_knowledge_graph(q)\n",
    "\n",
    "    if kg_result:\n",
    "        return {\n",
    "            \"kg_result\": kg_result,\n",
    "            \"route\": \"kg\",\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"kg_result\": None,\n",
    "            \"route\": \"rag\",\n",
    "        }\n",
    "\n",
    "\n",
    "def answer_from_kg_node(state: GraphState) -> Dict:\n",
    "    \"\"\"KG hitì¸ ê²½ìš°: KG ë°ì´í„°ë¡œ ë°”ë¡œ answer ìƒì„±.\"\"\"\n",
    "    msgs = state.get(\"messages\", [])\n",
    "    kg_result = state.get(\"kg_result\")\n",
    "    if not kg_result:\n",
    "        ai = AIMessage(content=\"ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\")\n",
    "        return {\"answer\": ai.content, \"messages\": msgs + [ai]}\n",
    "\n",
    "    text = format_kg_result_for_answer(kg_result)\n",
    "    ai = AIMessage(content=text)\n",
    "    return {\"answer\": text, \"messages\": msgs + [ai]}\n",
    "\n",
    "\n",
    "def retrieve_rag_node(state: GraphState) -> Dict:\n",
    "    \"\"\"KGì— ì—†ìœ¼ë©´: ê³ ê¸‰ retrieverë¡œ ë¬¸ì„œ ê²€ìƒ‰.\"\"\"\n",
    "    q = state.get(\"question\", \"\")\n",
    "    docs = compression_retriever.invoke(q)\n",
    "    return {\"rag_docs\": docs}\n",
    "\n",
    "\n",
    "def generate_rag_answer_node(state: GraphState) -> Dict:\n",
    "    \"\"\"retriever ê²°ê³¼ + prompt + LLM â†’ answer.\"\"\"\n",
    "    msgs = state.get(\"messages\", [])\n",
    "    q = state.get(\"question\", \"\")\n",
    "    docs = state.get(\"rag_docs\") or []\n",
    "\n",
    "    context = format_docs(docs)\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    answer = chain.invoke(\n",
    "        {\n",
    "            \"context\": context,\n",
    "            \"question\": q,\n",
    "            \"chat_history\": msgs,  # historyë¥¼ promptì— ê·¸ëŒ€ë¡œ ë„£ì–´ë„ ë¨\n",
    "        }\n",
    "    )\n",
    "\n",
    "    ai = AIMessage(content=answer)\n",
    "    return {\"answer\": answer, \"messages\": msgs + [ai]}\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 5. ì¡°ê±´ ë¶„ê¸°\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def route_after_kg(state: GraphState) -> str:\n",
    "    \"\"\"search_kg_node ì´í›„ ì–´ë””ë¡œ ê°ˆì§€ ê²°ì •.\"\"\"\n",
    "    route = state.get(\"route\")\n",
    "    if route == \"kg\":\n",
    "        return \"kg\"\n",
    "    return \"rag\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 6. ê·¸ë˜í”„ êµ¬ì„±\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def build_graph():\n",
    "    g = StateGraph(GraphState)\n",
    "\n",
    "    g.add_node(\"prepare_question\", prepare_question_node)\n",
    "    g.add_node(\"search_kg\", search_kg_node)\n",
    "    g.add_node(\"answer_from_kg\", answer_from_kg_node)\n",
    "    g.add_node(\"retrieve_rag\", retrieve_rag_node)\n",
    "    g.add_node(\"generate_rag_answer\", generate_rag_answer_node)\n",
    "\n",
    "    g.set_entry_point(\"prepare_question\")\n",
    "\n",
    "    g.add_edge(\"prepare_question\", \"search_kg\")\n",
    "\n",
    "    # KG hit ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°\n",
    "    g.add_conditional_edges(\n",
    "        \"search_kg\",\n",
    "        route_after_kg,\n",
    "        {\n",
    "            \"kg\": \"answer_from_kg\",\n",
    "            \"rag\": \"retrieve_rag\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # RAG path\n",
    "    g.add_edge(\"retrieve_rag\", \"generate_rag_answer\")\n",
    "    g.add_edge(\"answer_from_kg\", END)\n",
    "    g.add_edge(\"generate_rag_answer\", END)\n",
    "\n",
    "    app = g.compile(checkpointer=MemorySaver())\n",
    "    return app\n",
    "\n",
    "\n",
    "graph = build_graph()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    internal = graph.get_graph()\n",
    "    print(internal.draw_mermaid())\n",
    "    internal.draw_mermaid_png(\"kg_rag_pipeline.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52704d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    internal = graph.get_graph()\n",
    "    print(internal.draw_mermaid())\n",
    "    internal.draw_mermaid_png(\"video_rag_detailed_pipeline.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64306a40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c6efcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-bot-vbdTYmCJ-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
