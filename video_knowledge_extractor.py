
import json
import glob
from pathlib import Path
from dotenv import load_dotenv
import unicodedata
import re
import difflib
from openai import OpenAI

load_dotenv()
client = OpenAI()

video_url_map = {
    "01_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #29 | ê³¼ê±°ì— ìˆì—ˆë˜ ì¼ ì„¤ëª…í•  ë•Œ  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=R_-pgaQYaYQ",
    "02_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #28 | ë§ˆë²•ê³¼ ê°™ì€ that | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=008886a-lQI",
    "03_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° ##27 | ìˆ˜ë™íƒœë¥¼ thatê³¼ ì—°ê²°í•˜ê¸°! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=r3qBF9dMz10",
    "04_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #26 | ìˆ˜ë™íƒœ í•µì‹¬ íŒŒì•…! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=bGl_7acUnNk",
    "05_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #25 | haveë¥¼ ê¹Šì´, ìì—°ìŠ¤ëŸ½ê²Œ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=jOb8mznvX48",
    "06_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #24 | í˜„ì¬ì™„ë£Œ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=RgNbTRRt78Y",
    "07_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #23 | that ë’¤ì— ì¡°ë™ì‚¬ ì“°ê¸°  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=OaTujaboBf8",
    "08_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #22 | that ë’¤ì— ì§„í–‰í˜•ì„ ê°€ì§€ê³  ë§Œë“¤ì–´ ë³´ì  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=NXbGg9nxpdk",
    "09_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #21 | ë˜ ë‹¤ë¥¸ that | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=k3-666q27Ps",
    "10_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #20 | thatìœ¼ë¡œ ë¬¸ì¥ì„ ê¸¸ê²Œ! (3) | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=JMfB_2pfqCA",
    "11_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #19 | that ìœ¼ë¡œ ë¬¸ì¥ ê¸¸ê²Œ ë§Œë“¤ê¸°(2) | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=gHrI6qhbziI",
    "12_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#18 | that ìœ¼ë¡œ ë¬¸ì¥ ê¸¸ê²Œ ë§Œë“¤ê¸°(1) | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=nHbEN7KEmmE",
    "13_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #17 | canì„ be able toë¡œ ë°”ê¾¸ê¸°! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=CGN1TdvhkvY",
    "14_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #16 | ë™ëª…ì‚¬ë¡œ ì£¼ì–´ ê¸¸ê²Œ ë§Œë“¤ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=jzJzdoBdeAc",
    "15_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #15 | ëª©ì ì„ ë‚˜íƒ€ë‚´ëŠ” to ë™ì‚¬ì›í˜• | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=7CvXgPmdD9s",
    "16_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #14 | ë¯¸ë˜ì™€ ê³¼ê±°ì—ë„ ì§„í–‰í˜•ì„ ì“´ë‹¤! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=7ot7hY8wm4Q",
    "17_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #13 | í˜„ì¬ ì§„í–‰í˜•ì˜ ë‹¤ì–‘í•œ ì“°ì„.json": "https://www.youtube.com/watch?v=j87QB9EZZrY",
    "18_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #12 | '~ì— ìˆë‹¤'ë¥¼ ëœ»í•˜ëŠ” be ë™ì‚¬ ì—°ìŠµ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=h_Yv5bX8p8k",
    "19_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#11 | ë‹¤ì–‘í•œ be ë™ì‚¬ í˜•íƒœ ì—°ìŠµ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=WS5hLZV7Lb4",
    "20_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#10 | ë°”ì˜ë‹¤ëŠ” busyê°€ ì•„ë‹ˆë‹¤! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=V8e_cwY7VTs",
    "21_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#9 | ifë¡œ ë¬¸ì¥ ê¸¸ê²Œ ë§Œë“¤ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=hh9pAAS-gho",
    "22_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#8 | '~í•˜ê¸°ë¥¼' to ë™ì‚¬ì›í˜• ì—°ìŠµ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=TK1HL_27g6U",
    "23_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #5  | ì˜ë¬¸ë¬¸ ê¸¸ê²Œ ë§Œë“¤ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=EXKS6rZHbbA",
    "24_NEW ì´ì‹œì›ì˜ ê¸°ì´ˆ ì˜ì–´ íšŒí™” ê°•ì˜í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #4 | ê³¼ê±° ì‹œì œ ë§ˆìŠ¤í„° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=G_SNroMhJTQ",
    "25_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#3 | And ë¡œ ë¬¸ì¥ì„ ê¸¸ê²Œ!  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=VJeidy58uJQ",
    "26_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#1 | ì˜ì–´ëŠ” ë‹¨ì–´ì˜ ì—°ê²° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=oLIpoVoDgTo",
    "27_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#2 | ë¯¸ë˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” will | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=KQbWy6j_TFA",
    "28_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #30 | ë§ì´ ì“°ëŠ” ë™ì‚¬ put / get / take | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=1dIALFMvJlA",
    "29_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#6 | ì˜ë¬¸ë¬¸ ì§ˆë¬¸ì— ë‹µí•˜ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=a_eNZ4ZVwxc",
    "30_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #7 | ê°€ëŠ¥ê³¼ í—ˆë½ì˜ can | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=kYx8f4U4-jo"
}

def normalize_filename(name):
    """íŒŒì¼ëª… ì •ê·œí™”"""
    # NFC ì •ê·œí™” (ë§¥/ë¦¬ëˆ…ìŠ¤ íŒŒì¼ì‹œìŠ¤í…œ í˜¸í™˜)
    name = unicodedata.normalize("NFC", name)
    # ê³µë°±ê³¼ íŠ¹ìˆ˜ë¬¸ì í†µì¼
    name = name.replace("ï½œ", "|").replace("ï¼ƒ", "#")
    name = re.sub(r"\s+", " ", name.strip())
    return name

def get_video_url(video_filename):
    """íŒŒì¼ëª…ìœ¼ë¡œ ë¹„ë””ì˜¤ URL ì°¾ê¸° (ìœ ì‚¬ë„ ë§¤ì¹­)"""
    normalized_filename = normalize_filename(video_filename)

    # ì •ê·œí™”ëœ í‚¤ ëª©ë¡
    normalized_keys = {normalize_filename(k): k for k in video_url_map.keys()}

    # ì™„ì „ ì¼ì¹˜ í™•ì¸
    if normalized_filename in normalized_keys:
        original_key = normalized_keys[normalized_filename]
        return video_url_map[original_key]

    # ìœ ì‚¬ë„ ë§¤ì¹­
    keys = list(normalized_keys.keys())
    match = difflib.get_close_matches(normalized_filename, keys, n=1, cutoff=0.6)

    if match:
        original_key = normalized_keys[match[0]]
        return video_url_map[original_key]

    return "URL_ì—†ìŒ"


# ============================================================
# Step 1: ì˜¤ì¸ì‹ íŒ¨í„´ ì¶”ì¶œ (LLM ì‚¬ìš©)
# ============================================================
def extract_corrections_from_transcript(transcript_text: str) -> dict:
    """Whisper ì˜¤ì¸ì‹ íŒ¨í„´ ì¶”ì¶œ"""
    all_corrections = {}

    chunk_size = 1500
    for i in range(0, len(transcript_text), chunk_size):
        chunk = transcript_text[i:i + chunk_size]
        prompt = f"""ë‹¹ì‹ ì€ ì˜ì–´ ê°•ì˜ ìŒì„±ì¸ì‹ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” í•œêµ­ì¸ ì˜ì–´ ì„ ìƒë‹˜ì˜ ê°•ì˜ë¥¼ Whisperë¡œ ìŒì„±ì¸ì‹í•œ ê²°ê³¼ì…ë‹ˆë‹¤.
ì„ ìƒë‹˜ì´ ì˜ì–´ ë‹¨ì–´ë¥¼ ë°œìŒí–ˆëŠ”ë° í•œê¸€ë¡œ ì˜ëª» ì¸ì‹ëœ ë¶€ë¶„ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
 - "beë™ì‚¬" â†’ "ë¹„ë™ì‚¬"
 - "have" â†’ "í•´ë¸Œ", "í•´ë¶€"
 - "was" â†’ "ì›Œì¦ˆ"
 - "been" â†’ "ë¹ˆ"

ì˜ëª»ëœ ì˜ˆì‹œ (ì´ê±´ í•˜ì§€ ë§ˆì„¸ìš”):
- "ì‚¬ëŒë“¤ì´" â†’ "people"  (ì´ê±´ ë²ˆì—­ì„)
- "ì˜¤ëŠ˜" â†’ "today" (ì´ê±´ ë²ˆì—­ì„)

íŠ¸ëœìŠ¤í¬ë¦½íŠ¸:
{chunk}

ìœ„ í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€ë¡œ ì˜ëª» ì¸ì‹ëœ ì˜ì–´ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ì„œ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
{{"í•œê¸€ì˜¤ì¸ì‹": "ì˜¬ë°”ë¥¸ì˜ì–´", ...}}
"""

        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0,
                max_tokens=800
            )
            corrections = json.loads(response.choices[0].message.content)
            all_corrections.update(corrections)
        except:
            continue

    return all_corrections


def apply_corrections(text: str, corrections: dict) -> str:
    """ì˜¤ì¸ì‹ íŒ¨í„´ì„ ì˜¬ë°”ë¥¸ ì˜ì–´ë¡œ êµì²´"""
    sorted_corrections = sorted(corrections.items(), key=lambda x: len(x[0]), reverse=True)
    for wrong, right in sorted_corrections:
        text = text.replace(wrong, right)
    return text


def apply_corrections_to_segments(segments: list, corrections: dict) -> list:
    """ê° segmentì˜ textì— ë³´ì • ì ìš©"""
    corrected_segments = []
    for seg in segments:
        corrected_seg = seg.copy()
        corrected_seg["text"] = apply_corrections(seg["text"], corrections)
        corrected_segments.append(corrected_seg)
    return corrected_segments


def extract_knowledge_structure(video_data, video_metadata, corrections: dict = None):
    """ì˜ìƒ í•˜ë‚˜ì—ì„œ ì§€ì‹ êµ¬ì¡° ì¶”ì¶œ"""

    segments = video_data.get('segments', [])

    # ë³´ì • ì ìš©
    if corrections:
        segments = apply_corrections_to_segments(segments, corrections)

    # ì „ì²´ transcript í•©ì¹˜ê¸° (ì‹œê°„ ì •ë³´ í¬í•¨)
    full_transcript = "\n".join([
        f"[{seg['start']:.1f}s ~ {seg['end']:.1f}s] {seg['text']}"
        for seg in segments
    ])

    # 8000ìê¹Œì§€ ì‚¬ìš© (ê¸°ì¡´ 3000 â†’ 8000)
    if len(full_transcript) > 8000:
        full_transcript = full_transcript[:8000]

    video_title = video_metadata.get('title', 'Unknown')

    prompt = f"""ë‹¹ì‹ ì€ ì˜ì–´ ê°•ì˜ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ "{video_title}" ì˜ì–´ ê°•ì˜ì˜ ì „ì²´ ìŠ¤í¬ë¦½íŠ¸ì…ë‹ˆë‹¤.

{full_transcript}

ì´ ê°•ì˜ì—ì„œ ë‹¤ë£¨ëŠ” ëª¨ë“  ë¬¸ë²• ì£¼ì œì™€ ì„¸ë¶€ ë‚´ìš©ì„ ë¶„ì„í•˜ì„¸ìš”.
í•˜ë‚˜ì˜ ì˜ìƒì—ì„œ ì—¬ëŸ¬ sub_topicì´ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ì˜ˆ: ê¸°ë³¸ ì„¤ëª…, ì˜ë¬¸ë¬¸, ë¶€ì •ë¬¸, ì˜ˆë¬¸ ì—°ìŠµ ë“±).

JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”:
{{
  "main_topic": "ì´ ì˜ìƒì˜ í•µì‹¬ ë¬¸ë²• ì£¼ì œ (ì˜ˆ: í˜„ì¬ì§„í–‰í˜•, beë™ì‚¬, thatì ˆ)",
  "definition": "ì´ ë¬¸ë²•ì´ ë¬´ì—‡ì¸ì§€ ëª…í™•í•˜ê²Œ ìš”ì•½ (ì˜ˆ: 'ìˆ˜ë™íƒœëŠ” ì£¼ì–´ê°€ ë™ì‘ì„ ë‹¹í•˜ëŠ” ê²ƒì„ í‘œí˜„í•˜ëŠ” ë¬¸ë²•ì´ë‹¤. be + ê³¼ê±°ë¶„ì‚¬ í˜•íƒœë¡œ ë§Œë“ ë‹¤.')",
  "teacher_tip": "ì„ ìƒë‹˜ì´ ì´ ë¬¸ë²•ì„ ì‰½ê²Œ ì´í•´ì‹œí‚¤ê¸° ìœ„í•´ ì‚¬ìš©í•œ ë¹„ìœ ë‚˜ í•µì‹¬ ì„¤ëª…. ë°˜ë“œì‹œ ë¬¸ë²• ê°œë…ê³¼ ì—°ê²°ëœ ë‚´ìš©ì´ì–´ì•¼ í•¨. (ì˜ˆ: ìˆ˜ë™íƒœ - 'ì£¼ì–´ ì…ì¥ì—ì„œ ë‹¹í•˜ëŠ” ê±°ì˜ˆìš”. I wear a watchë©´ ì‹œê³„ ì…ì¥ì—ì„œëŠ” The watch is wornì´ ë˜ëŠ” ê±°ì£ ')",
  "sub_topics": [
    {{
      "id": "ê³ ìœ ID (snake_case, ì˜ˆ: present_continuous_question)",
      "title": "ì„œë¸Œí† í”½ ì œëª© (ì˜ˆ: í˜„ì¬ì§„í–‰í˜• ì˜ë¬¸ë¬¸)",
      "concept": "ì´ ì„œë¸Œí† í”½ì˜ í•µì‹¬ ê°œë… ìš”ì•½ (1-2ë¬¸ì¥)",
      "teacher_explanation": "ì„ ìƒë‹˜ì´ ì´ ë¶€ë¶„ì„ ì„¤ëª…í•  ë•Œ í•µì‹¬ í•œë§ˆë”” (ì§§ê³  ì´í•´í•˜ê¸° ì‰¬ìš´ í‘œí˜„)",
      "examples": ["I have studied for two years. (ë‚˜ëŠ” 2ë…„ ë™ì•ˆ ê³µë¶€í•´ì™”ì–´)", "We have met before. (ìš°ë¦¬ ì „ì— ë§Œë‚œ ì  ìˆì–´)"],
      "video_segments": [
        {{
          "start_time": 28.5,
          "end_time": 46.0,
          "description": "ì´ êµ¬ê°„ì—ì„œ ë‹¤ë£¨ëŠ” ë‚´ìš© ìš”ì•½"
        }}
      ]
    }}
  ],
  "related_topics": ["ì—°ê´€ ë¬¸ë²• ì£¼ì œ1", "ì—°ê´€ ë¬¸ë²• ì£¼ì œ2"]
}}

ì¤‘ìš”:
- definitionì€ ë¬¸ë²• ê°œë…ì„ ëª…í™•í•˜ê²Œ ì •ì˜í•˜ì„¸ìš” (ìŠ¤í¬ë¦½íŠ¸ ë³µì‚¬ X, ìš”ì•½ O)
- teacher_tipì€ ë°˜ë“œì‹œ ë¬¸ë²• ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ë¹„ìœ /íŒì´ì–´ì•¼ í•¨ (ë‹¨ìˆœ ë¬¸ì¥ ì˜ˆì‹œ X)
  - ì¢‹ì€ ì˜ˆ: "ìˆ˜ë™íƒœëŠ” ì£¼ì–´ê°€ ë™ì‘ì„ ë‹¹í•˜ëŠ” ì…ì¥ì´ì—ìš”"
  - ë‚˜ìœ ì˜ˆ: "ë‚´ê°€ ì‹œê³„ë¥¼ ì°¨ë©´, ì‹œê³„ ì…ì¥ì—ì„œëŠ” ì°¨ì§€ëŠ” ê±°ì˜ˆìš”" (ì´ê±´ ê·¸ëƒ¥ ì˜ˆì‹œë¬¸)
- teacher_explanationë„ ë¬¸ë²• ê°œë… ì„¤ëª…ì´ì–´ì•¼ í•¨ (1-2ë¬¸ì¥)
- sub_topicsëŠ” ìµœì†Œ 2ê°œ ì´ìƒ ì¶”ì¶œí•˜ì„¸ìš”
- video_segmentsì˜ ì‹œê°„ì€ ìŠ¤í¬ë¦½íŠ¸ì˜ [ì‹œê°„] ì •ë³´ë¥¼ ì°¸ê³ í•˜ì„¸ìš”
- examplesëŠ” ì‹¤ì œ ê°•ì˜ì—ì„œ ë‚˜ì˜¨ ì™„ì „í•œ ì˜ì–´ ë¬¸ì¥ì„ ì‚¬ìš©í•˜ì„¸ìš”
- ê° ì˜ˆë¬¸ì€ ë°˜ë“œì‹œ "ì˜ì–´ ë¬¸ì¥. (í•œêµ­ì–´ ë²ˆì—­)" í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0,
            max_tokens=4000
        )
        return json.loads(response.choices[0].message.content)
    except json.JSONDecodeError as e:
        print(f"âŒ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        return None
    except Exception as e:
        print(f"âŒ API ì—ëŸ¬: {e}")
        return None

def build_knowledge_graph():
    """ëª¨ë“  ì˜ìƒì—ì„œ ì§€ì‹ ê·¸ë˜í”„ ìƒì„±"""

    json_files = glob.glob("/data/edutem/sooine/rag_bot/merged_data/*.json")

    all_knowledge = {}

    print(f"ğŸ“š ì´ {len(json_files)}ê°œ ì˜ìƒ ì²˜ë¦¬ ì¤‘...")

    for i, json_file in enumerate(json_files, 1):
        filename = Path(json_file).name
        print(f"\n[{i}/{len(json_files)}] {filename[:50]}...")

        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                video_data = json.load(f)

            # ë¹„ë””ì˜¤ URL ì°¾ê¸° (ìœ ì‚¬ë„ ë§¤ì¹­)
            video_url = get_video_url(filename)

            if video_url == "URL_ì—†ìŒ":
                print(f"  âš ï¸  URLì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")

            # Step 1: ì˜¤ì¸ì‹ ë³´ì •
            segments = video_data.get('segments', [])
            # STT + OCR í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
            transcript_text = " ".join([
                seg.get("text", "") + (" [í™”ë©´: " + " ".join(seg.get("screen_text", [])) + "]" if seg.get("screen_text") else "")
                for seg in segments
            ])
            print(f"  ğŸ”§ ì˜¤ì¸ì‹ ë³´ì • ì¤‘...")
            corrections = extract_corrections_from_transcript(transcript_text)
            if corrections:
                print(f"     {len(corrections)}ê°œ íŒ¨í„´ ë°œê²¬")

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            video_metadata = {
                'title': filename.replace('.json', ''),
                'filename': filename,
                'video_url': video_url
            }

            # Step 2: ì§€ì‹ êµ¬ì¡° ì¶”ì¶œ (ë³´ì •ëœ í…ìŠ¤íŠ¸ë¡œ)
            print(f"  ğŸ“– ì§€ì‹ êµ¬ì¡° ì¶”ì¶œ ì¤‘...")
            knowledge = extract_knowledge_structure(video_data, video_metadata, corrections)

            if not knowledge:
                print(f"  âš ï¸  ì§€ì‹ ì¶”ì¶œ ì‹¤íŒ¨")
                continue

            main_topic = knowledge.get('main_topic', 'Unknown')
            definition = knowledge.get('definition', '')
            teacher_tip = knowledge.get('teacher_tip', '')
            sub_topics = knowledge.get('sub_topics', [])

            print(f"  âœ… ì£¼ì œ: {main_topic}")
            print(f"     ì„œë¸Œí† í”½ {len(sub_topics)}ê°œ ë°œê²¬")

            related_topics = knowledge.get('related_topics', [])

            # ì§€ì‹ ë² ì´ìŠ¤ì— ì¶”ê°€
            if main_topic not in all_knowledge:
                all_knowledge[main_topic] = {
                    "definition": definition,
                    "teacher_tip": teacher_tip,
                    "sub_topics": {},
                    "videos": [],
                    "related_topics": set()
                }
            else:
                # ë¹„ì–´ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸
                if not all_knowledge[main_topic]["definition"] and definition:
                    all_knowledge[main_topic]["definition"] = definition
                if not all_knowledge[main_topic]["teacher_tip"] and teacher_tip:
                    all_knowledge[main_topic]["teacher_tip"] = teacher_tip

            # related_topics ë³‘í•©
            all_knowledge[main_topic]["related_topics"].update(related_topics)

            # ì„œë¸Œí† í”½ ë³‘í•©
            for sub in sub_topics:
                sub_id = sub.get('id', 'unknown')
                if sub_id not in all_knowledge[main_topic]['sub_topics']:
                    all_knowledge[main_topic]['sub_topics'][sub_id] = {
                        "title": sub.get('title', ''),
                        "concept": sub.get('concept', ''),
                        "teacher_explanation": sub.get('teacher_explanation', ''),
                        "examples": sub.get('examples', []),
                        "video_segments": []
                    }

                # ì˜ìƒ ì„¸ê·¸ë¨¼íŠ¸ ì¶”ê°€
                for seg in sub.get('video_segments', []):
                    all_knowledge[main_topic]['sub_topics'][sub_id]['video_segments'].append({
                        "video_url": video_url,
                        "video_title": video_metadata['title'],
                        "filename": filename,
                        "start_time": seg.get('start_time', 0),
                        "end_time": seg.get('end_time', 0),
                        "description": seg.get('description', '')
                    })

            # ë¹„ë””ì˜¤ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            all_knowledge[main_topic]['videos'].append({
                "url": video_url,
                "title": video_metadata['title'],
                "filename": filename
            })

        except Exception as e:
            print(f"  âŒ ì—ëŸ¬: {e}")
            import traceback
            traceback.print_exc()
            continue

    return all_knowledge

if __name__ == "__main__":
    print("ğŸš€ ì§€ì‹ ê·¸ë˜í”„ ìƒì„± ì‹œì‘...\n")

    knowledge_graph = build_knowledge_graph()

    # setì„ listë¡œ ë³€í™˜ (JSON ì €ì¥ìš©)
    for topic in knowledge_graph:
        if isinstance(knowledge_graph[topic].get("related_topics"), set):
            knowledge_graph[topic]["related_topics"] = list(knowledge_graph[topic]["related_topics"])

    # ì €ì¥
    output_path = "/data/edutem/sooine/rag_bot/knowledge_graph.json"
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(knowledge_graph, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ì™„ë£Œ! ì €ì¥ ìœ„ì¹˜: {output_path}")
    print(f"\nğŸ“Š í†µê³„:")
    print(f"  - ì´ ì£¼ì œ: {len(knowledge_graph)}ê°œ")
    for topic, data in knowledge_graph.items():
        print(f"  - {topic}: ì„œë¸Œí† í”½ {len(data['sub_topics'])}ê°œ, ì˜ìƒ {len(data['videos'])}ê°œ")