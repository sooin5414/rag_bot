import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
import streamlit.components.v1 as components
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from dotenv import load_dotenv
import openai
import json
import os
from rapidfuzz import process, fuzz
import unicodedata
load_dotenv()

# ============================================================
# 1. ì´ˆê¸° ì„¤ì •
# ============================================================

st.set_page_config(
    page_title="ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸",
    page_icon="ğŸ“š",
)

st.title("ğŸ“š ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸")
st.markdown("ì˜ìƒ ê¸°ë°˜ ë§ì¶¤í˜• í•™ìŠµ ì‹œìŠ¤í…œ")

# ============================================================
# 2. ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ (ìºì‹œ)
# ============================================================

@st.cache_resource
def load_vectorstore():    
    with st.spinner("ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì¤‘..."):
            embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
        
        # ì´ë¯¸ ë§Œë“¤ì–´ì§„ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ
            vectorstore = Chroma(
            persist_directory="../chroma_db",
            embedding_function=embeddings,
            )
    
    return vectorstore

vectorstore = load_vectorstore()
retriever = vectorstore.as_retriever(search_kwargs={"k":5})

# ì§€ì‹ ê·¸ë˜í”„ ë¡œë“œ (íŒŒì¼ ìˆ˜ì • ì‹œê°„ ê¸°ë°˜ ìºì‹œ)
@st.cache_resource
def load_knowledge_graph(_mtime):
    with open('/data/edutem/sooine/rag_bot/knowledge_graph.json', 'r', encoding='utf-8') as f:
        return json.load(f)

# íŒŒì¼ ìˆ˜ì • ì‹œê°„ì´ ë°”ë€Œë©´ ìºì‹œ ë¬´íš¨í™”
kg_path = '/data/edutem/sooine/rag_bot/knowledge_graph.json'
kg_mtime = os.path.getmtime(kg_path)
knowledge_graph = load_knowledge_graph(kg_mtime)


# ============================================================
# ì§€ì‹ ê·¸ë˜í”„ ê²€ìƒ‰ í•¨ìˆ˜
# ============================================================
def rewrite_query(query):
    # ì‹¤ì œ í† í”½ ëª©ë¡ì„ í”„ë¡¬í”„íŠ¸ì— ì œê³µ
    available_topics = list(knowledge_graph.keys())
    topics_str = ", ".join(available_topics)

    prompt = f"""ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•´ì„œ ì•„ë˜ í† í”½ ëª©ë¡ ì¤‘ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ê²ƒì„ ì„ íƒí•˜ì„¸ìš”.
                ì‚¬ìš© ê°€ëŠ¥í•œ í† í”½ ëª©ë¡:
                {topics_str}

                ì‚¬ìš©ì ì§ˆë¬¸: {query}

                ê·œì¹™:
                - ìœ„ ëª©ë¡ì— ìˆëŠ” í† í”½ ì¤‘ì—ì„œë§Œ ì„ íƒí•˜ì„¸ìš”
                - ì§ˆë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í† í”½ë§Œ ì„ íƒí•˜ì„¸ìš”
                - ë¹„ìŠ·í•˜ì§€ë§Œ ë‹¤ë¥¸ ê°œë…ì´ë©´ "ì—†ìŒ"ì„ ì¶œë ¥í•˜ì„¸ìš” (ì˜ˆ: "í˜„ì¬ ì‹œì œ" â‰  "í˜„ì¬ì§„í–‰í˜•")
                - ëª©ë¡ì— ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” í† í”½ì´ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ "ì—†ìŒ"ì„ ì¶œë ¥í•˜ì„¸ìš”

                ì¶œë ¥: í† í”½ ì´ë¦„ ë˜ëŠ” "ì—†ìŒ" (ì„¤ëª… ì—†ì´)"""
    return llm.invoke(prompt).content.strip()

def normalize(s):
    return unicodedata.normalize("NFC", s.lower().replace(" ", ""))

def fuzzy_match_topic(query, topic_list):
    #ëŒ€ì¶© ë¹„ìŠ·í•œ ë¬¸ìì—´ë„ ë§¤ì¹­í•´ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜
    q = normalize(query)
    candidates = [normalize(t) for t in topic_list]
    match, score, idx = process.extractOne(q, candidates, scorer=fuzz.ratio)
    if score > 70:  # threshold ì¡°ì • ê°€ëŠ¥
        return topic_list[idx]
    return None

def search_in_knowledge_graph(query):
    """ì§€ì‹ ê·¸ë˜í”„ì—ì„œ í‚¤ì›Œë“œ/ì§ˆë¬¸ ê²€ìƒ‰"""
    query_lower = query.lower().strip()
    topic_list = list(knowledge_graph.keys())

    # 1ë‹¨ê³„: ì •í™•í•œ ì¼ì¹˜
    for main_topic in topic_list:
        if main_topic.lower() == query_lower:
            return {
                "type": "main_topic",
                "main_topic": main_topic,
                "data": knowledge_graph[main_topic]
            }

    # 2ë‹¨ê³„: ë¶€ë¶„ ì¼ì¹˜
    for main_topic in topic_list:
        if query_lower in main_topic.lower() or main_topic.lower() in query_lower:
            return {
                "type": "main_topic",
                "main_topic": main_topic,
                "data": knowledge_graph[main_topic]
            }

    # 3ë‹¨ê³„: Sub-topic ê²€ìƒ‰ (title, concept, examples)
    best_match = None
    max_score = 0

    for main_topic, topic_data in knowledge_graph.items():
        for sub_id, sub_data in topic_data['sub_topics'].items():
            score = 0

            # title ë§¤ì¹­
            if query_lower in sub_data['title'].lower():
                score += 3

            # concept ë§¤ì¹­
            if query_lower in sub_data['concept'].lower():
                score += 2

            # examples ë§¤ì¹­
            for example in sub_data.get('examples', []):
                if query_lower in example.lower():
                    score += 1
                    break

            if score > max_score:
                max_score = score
                best_match = {
                    "type": "sub_topic",
                    "main_topic": main_topic,
                    "sub_topic_id": sub_id,
                    "data": sub_data,
                    "score": score
                }

    if best_match and max_score >= 1:
        return best_match

    # 4ë‹¨ê³„: Fuzzy match (ë§ˆì§€ë§‰ ìˆ˜ë‹¨)
    best = fuzzy_match_topic(query, topic_list)
    if best:
        return {
            "type": "main_topic",
            "main_topic": best,
            "data": knowledge_graph[best]
        }

    # 5ë‹¨ê³„: ë§¤ì¹­ ì‹¤íŒ¨
    return None

# ============================================================
# ì„¸ì…˜ ì €ì¥ì†Œ
# ============================================================

if "store" not in st.session_state:
    st.session_state["store"] = {}
    
# ì„¸ì…˜ ID
if "session_id" not in st.session_state:
    st.session_state["session_id"] = "default_user"
    
if "messages" not in st.session_state:
    st.session_state["messages"] = []

if "mode" not in st.session_state:
    st.session_state["mode"] = "search"

def get_session_history(session_id):
    if session_id not in st.session_state["store"]:
        st.session_state["store"][session_id] = ChatMessageHistory()
    return st.session_state["store"][session_id]

# ============================================================
# ì²´ì¸ ìƒì„±
# ============================================================

llm = ChatOpenAI(model="gpt-4o", temperature=0)
prompt = ChatPromptTemplate.from_messages([
        ("system", """ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤.
    
        ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:
        {context}

        ì´ì „ ëŒ€í™” ê¸°ë¡:
        {chat_history}

        ë‹µë³€ì€ ì¹œì ˆí•˜ê³  ì´í•´í•˜ê¸° ì‰½ê²Œ ì‘ì„±í•˜ì„¸ìš”."""),
            ("human", "{question}")
        ])


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)
    
# RAG ì²´ì¸
rag_chain = (
    {
        "context": lambda x: format_docs(retriever.invoke(x["question"])),
        "question": lambda x: x["question"],
        "chat_history": lambda x: x["chat_history"]
    }
    | prompt
    | llm
    | StrOutputParser()
)

def rag_with_chain(inputs):
    docs = retriever.invoke(inputs["question"])
    
    answer = (prompt | llm | StrOutputParser()).invoke({
        "context" : format_docs(docs),
        "question" : inputs["question"],
        "chat_history" : inputs["chat_history"]
    })
    return {"answer": answer, "source_docs": docs}


rag_chain = RunnableLambda(rag_with_chain) 

chain_with_history = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="question",
    history_messages_key="chat_history"
)

# ============================================================
# 3. ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ============================================================


# ============================================================
# 4. ì‚¬ì´ë“œë°”
# ============================================================

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # ëª¨ë“œ ì„ íƒ
    mode = st.radio(
        "ëª¨ë“œ ì„ íƒ",
        ["ğŸ” Search (ê²€ìƒ‰)", "ğŸ“ Quiz (ë¬¸ì œ)", "ğŸ“– Review (ë³µìŠµ)"],
        index=0
    )
    
    if "Search" in mode:
        st.session_state["mode"] = "search"
    elif "Quiz" in mode:
        st.session_state["mode"] = "quiz"
    elif "Review" in mode:
        st.session_state["mode"] = "review"
    
    st.divider()
    
      # ê°€ìš© ì£¼ì œ (knowledge_graphì—ì„œ ë™ì ìœ¼ë¡œ ë¡œë“œ)
    st.markdown("### ğŸ“š í˜„ì¬ í•™ìŠµ ê°€ëŠ¥í•œ ì£¼ì œ")
    topic_list = list(knowledge_graph.keys())
    for topic in topic_list:
        st.markdown(f"- {topic}")
    
    st.divider()
    
    # ì´ˆê¸°í™” ë²„íŠ¼
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.rerun()
    
    st.divider()
    

# ============================================================
# 5. ë©”ì¸ ì˜ì—­
# ============================================================

session_id = st.session_state["session_id"]
# ëŒ€í™” ê¸°ë¡ í‘œì‹œ
history = get_session_history(session_id)
for msg in reversed(history.messages):
    role = "user" if msg.type == "human" else "assistant"
    with st.chat_message(role):
        st.markdown(msg.content)
        

# ì‚¬ìš©ì ì…ë ¥    
user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
 
if user_input:      
    with st.chat_message("user"):
        st.markdown(user_input)
    
    # AI ì‘ë‹µ
    with st.chat_message("assistant"):
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            
            # ============================================================
            # Search ëª¨ë“œ
            # ============================================================
            if st.session_state["mode"] == "search":

                # 1ë‹¨ê³„: ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ë¨¼ì € ê²€ìƒ‰
                rewritten = rewrite_query(user_input)

                # "ì—†ìŒ"ì´ë©´ ì§€ì‹ ê·¸ë˜í”„ ê²€ìƒ‰ ìŠ¤í‚µí•˜ê³  ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ
                kg_result = None
                if rewritten and rewritten != "ì—†ìŒ":
                    kg_result = search_in_knowledge_graph(rewritten)

                if kg_result:
                    # ========== ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ì°¾ìŒ ==========
                    main_topic = kg_result['main_topic']
                    topic_data = kg_result['data'] if kg_result['type'] == 'main_topic' else knowledge_graph[main_topic]

                    # 0ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ë§¤ì¹­ëœ í† í”½ì´ ì•„ë‹Œ ì›ë˜ ì§ˆë¬¸ì˜ í‚¤ì›Œë“œ)
                    keyword_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•µì‹¬ ë¬¸ë²• í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

                            ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

                            ê·œì¹™:
                            - ì§ˆë¬¸ í˜•ì‹ ì œê±° (ë­ì•¼?, ì•Œë ¤ì¤˜, ì„¤ëª…í•´ì¤˜ ë“±)
                            - í•µì‹¬ ë¬¸ë²• ìš©ì–´ë§Œ ì¶”ì¶œ
                            - ì˜ˆ: "ifê°€ ë­ì•¼?" â†’ "if"
                            - ì˜ˆ: "to ë¶€ì •ì‚¬ ì„¤ëª…í•´ì¤˜" â†’ "to ë¶€ì •ì‚¬"

                            ì¶œë ¥: í‚¤ì›Œë“œë§Œ (ì„¤ëª… ì—†ì´)"""
                    topic_keyword = llm.invoke(keyword_prompt).content.strip()

                    # 1ë‹¨ê³„: LLMì´ ì¼ë°˜ì ì¸ ê°œë… ì„¤ëª… (í‚¤ì›Œë“œ ê¸°ì¤€, ë‹¤ì–‘í•œ ìš©ë²• í¬í•¨)
                    st.markdown(f"## ğŸ’¡ {topic_keyword}")
                    with st.spinner("ì„¤ëª… ìƒì„± ì¤‘..."):
                        explain_prompt = f"""'{topic_keyword}'ì´ ë¬´ì—‡ì¸ì§€ í•µì‹¬ë§Œ ê°„ë‹¨íˆ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”.
                         ë§Œì•½ '{topic_keyword}'ì´ ì—¬ëŸ¬ ìš©ë²•ìœ¼ë¡œ ì“°ì¼ ìˆ˜ ìˆë‹¤ë©´ ê°„ë‹¨íˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”."""
                        explanation = llm.invoke(explain_prompt).content.strip()
                    st.write(explanation)

                    # 2ë‹¨ê³„: í•´ë‹¹ ê°•ì˜ ì˜ìƒ (RAGë¡œ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ êµ¬ê°„ ì°¾ê¸°)
                    st.markdown("---")
                    st.success(f"âœ… ì´ êµì¬ì—ì„œ '{main_topic}'ì„(ë¥¼) ë‹¤ë£¨ê³  ìˆì–´ìš”!")

                    # ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í•´ë‹¹ í† í”½ì˜ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ êµ¬ê°„ ì°¾ê¸°
                    rag_docs = retriever.invoke(main_topic)
                    if rag_docs:
                        st.markdown("### ğŸ“º ê°•ì˜ ì˜ìƒ")
                        best_doc = rag_docs[0]
                        url = best_doc.metadata.get('video_url', '')
                        start = int(float(best_doc.metadata.get('start_time', 0)))
                        end = int(float(best_doc.metadata.get('end_time', 0)))

                        if url:
                            if "watch?v=" in url:
                                video_id = url.split("watch?v=")[-1].split("&")[0]
                            elif "youtu.be/" in url:
                                video_id = url.split("youtu.be/")[-1].split("?")[0]
                            else:
                                video_id = url

                            embed_url = f"https://www.youtube.com/embed/{video_id}?start={start}&end={end}"
                            st.components.v1.iframe(embed_url, width=800, height=450)

                    # 3ë‹¨ê³„: ê´€ë ¨ ì£¼ì œ
                    st.markdown("---")
                    available_topics = list(knowledge_graph.keys())

                    # 3-1: ì§€ì‹ê·¸ë˜í”„ì˜ related_topics (fuzzy ë§¤ì¹­)
                    kg_related_raw = topic_data.get('related_topics', [])
                    kg_related = []
                    for rel in kg_related_raw:
                        if rel == main_topic:
                            continue
                        # ì •í™•íˆ ìˆìœ¼ë©´ ê·¸ëŒ€ë¡œ
                        if rel in knowledge_graph:
                            kg_related.append(rel)
                        else:
                            # fuzzy ë§¤ì¹­ ì‹œë„
                            for kg_topic in available_topics:
                                if rel.lower() in kg_topic.lower() or kg_topic.lower() in rel.lower():
                                    if kg_topic != main_topic:
                                        kg_related.append(kg_topic)
                                    break

                    # 3-2: LLM ì¶”ì²œ ê´€ë ¨ ì£¼ì œ (ì˜ì–´ ë¬¸ë²• ë§¥ë½ ëª…ì‹œ)
                    related_prompt = f"""'{main_topic}'ê³¼ ê´€ë ¨ëœ ì˜ì–´ ë¬¸ë²• ì£¼ì œ 5ê°œë¥¼ ë‚˜ì—´í•˜ì„¸ìš”.

ê·œì¹™:
- ì˜ì–´ ë¬¸ë²•/íšŒí™” ê´€ë ¨ ì£¼ì œë§Œ (í”„ë¡œê·¸ë˜ë° ê¸ˆì§€)
- ì˜¤ì§ ì£¼ì œ ì´ë¦„ë§Œ ì¶œë ¥ (ì„¤ëª…, ë²ˆí˜¸, ë¬¸ì¥ ê¸ˆì§€)
- ì½¤ë§ˆë¡œ êµ¬ë¶„
- ì˜ˆì‹œ ì¶œë ¥: í˜„ì¬ì§„í–‰í˜•, í˜„ì¬ì™„ë£Œ, ê³¼ê±°ì‹œì œ, beë™ì‚¬, ì¡°ë™ì‚¬

ì¶œë ¥:"""
                    llm_related_result = llm.invoke(related_prompt).content.strip()
                    llm_related = [t.strip() for t in llm_related_result.split(",")]

                    # í‘œì‹œ: ì§€ì‹ê·¸ë˜í”„ related_topics (RAGë¡œ ì˜ìƒ êµ¬ê°„ ì°¾ê¸°)
                    if kg_related:
                        st.markdown("### ğŸ”— ê´€ë ¨ ì£¼ì œ (êµì¬ ê¸°ì¤€)")
                        for rel in kg_related:
                            with st.expander(f"âœ… **{rel}** - ê°•ì˜ ìˆìŒ", expanded=False):
                                # RAGë¡œ í•´ë‹¹ í† í”½ì˜ ì˜ìƒ êµ¬ê°„ ì°¾ê¸°
                                rel_docs = retriever.invoke(rel)
                                if rel_docs:
                                    rel_best = rel_docs[0]
                                    url = rel_best.metadata.get('video_url', '')
                                    start = int(float(rel_best.metadata.get('start_time', 0)))
                                    end = int(float(rel_best.metadata.get('end_time', 0)))

                                    if url:
                                        if "watch?v=" in url:
                                            video_id = url.split("watch?v=")[-1].split("&")[0]
                                        elif "youtu.be/" in url:
                                            video_id = url.split("youtu.be/")[-1].split("?")[0]
                                        else:
                                            video_id = url

                                        embed_url = f"https://www.youtube.com/embed/{video_id}?start={start}&end={end}"
                                        st.components.v1.iframe(embed_url, width=700, height=400)

                    # í‘œì‹œ: LLM ì¶”ì²œ ê´€ë ¨ ì£¼ì œ (ì¤‘ë³µ í¬í•¨, ì „ë¶€ í‘œì‹œ)
                    st.markdown("### ğŸ¤– ê´€ë ¨ ì£¼ì œ (AI ì¶”ì²œ)")
                    shown_kg_topics = set()  # ì´ë¯¸ ë³´ì—¬ì¤€ ì§€ì‹ê·¸ë˜í”„ í† í”½ ì¶”ì 
                    for topic in llm_related:
                        if topic.lower() == main_topic.lower():
                            continue

                        # ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ë§¤ì¹­ í™•ì¸ (ì •í™•í•œ ë§¤ì¹­ ìš°ì„ )
                        matched_kg_topic = None
                        # 1) ì •í™•íˆ ì¼ì¹˜
                        for kg_topic in available_topics:
                            if topic.lower() == kg_topic.lower():
                                matched_kg_topic = kg_topic
                                break
                        # 2) ë¶€ë¶„ ì¼ì¹˜ (ì •í™•í•œ ë§¤ì¹­ ì—†ì„ ë•Œë§Œ)
                        if not matched_kg_topic:
                            for kg_topic in available_topics:
                                if topic.lower() in kg_topic.lower() or kg_topic.lower() in topic.lower():
                                    matched_kg_topic = kg_topic
                                    break

                        if matched_kg_topic:
                            # ì´ë¯¸ ë³´ì—¬ì¤€ í† í”½ì´ë©´ ìŠ¤í‚µ
                            if matched_kg_topic in shown_kg_topics:
                                continue
                            shown_kg_topics.add(matched_kg_topic)

                            with st.expander(f"âœ… **{topic}** â†’ {matched_kg_topic}", expanded=False):
                                # RAGë¡œ í•´ë‹¹ í† í”½ì˜ ì˜ìƒ êµ¬ê°„ ì°¾ê¸°
                                rel_docs = retriever.invoke(matched_kg_topic)
                                if rel_docs:
                                    rel_best = rel_docs[0]
                                    url = rel_best.metadata.get('video_url', '')
                                    start = int(float(rel_best.metadata.get('start_time', 0)))
                                    end = int(float(rel_best.metadata.get('end_time', 0)))

                                    if url:
                                        if "watch?v=" in url:
                                            video_id = url.split("watch?v=")[-1].split("&")[0]
                                        elif "youtu.be/" in url:
                                            video_id = url.split("youtu.be/")[-1].split("?")[0]
                                        else:
                                            video_id = url

                                        embed_url = f"https://www.youtube.com/embed/{video_id}?start={start}&end={end}"
                                        st.components.v1.iframe(embed_url, width=700, height=400)
                        else:
                            st.markdown(f"âŒ **{topic}** - í•´ë‹¹ ê°•ì˜ ì—†ìŒ")

                else:
                    # ========== ì§€ì‹ ê·¸ë˜í”„ì— ì •í™•íˆ ì—†ìŒ â†’ LLM ì„¤ëª… + ê´€ë ¨ ì£¼ì œ ì—°ê²° ==========

                    # 0ë‹¨ê³„: ì§ˆë¬¸ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ
                    keyword_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ í•µì‹¬ ë¬¸ë²• í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {user_input}

ê·œì¹™:
- ì§ˆë¬¸ í˜•ì‹ ì œê±° (ë­ì•¼?, ì•Œë ¤ì¤˜, ì„¤ëª…í•´ì¤˜ ë“±)
- í•µì‹¬ ë¬¸ë²• ìš©ì–´ë§Œ ì¶”ì¶œ
- ì˜ˆ: "í˜„ì¬ì‹œì œê°€ ë­ì•¼?" â†’ "í˜„ì¬ì‹œì œ"
- ì˜ˆ: "to ë¶€ì •ì‚¬ ì„¤ëª…í•´ì¤˜" â†’ "to ë¶€ì •ì‚¬"

ì¶œë ¥: í‚¤ì›Œë“œë§Œ (ì„¤ëª… ì—†ì´)"""
                    topic_keyword = llm.invoke(keyword_prompt).content.strip()

                    # 1ë‹¨ê³„: LLMì´ ì§ˆë¬¸ì— ëŒ€í•´ ì„¤ëª… (ê°„ë‹¨íˆ)
                    with st.spinner("ì„¤ëª… ìƒì„± ì¤‘..."):
                        explain_prompt = f"""'{topic_keyword}'ì´ ë¬´ì—‡ì¸ì§€ í•µì‹¬ë§Œ ê°„ë‹¨íˆ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""
                        explanation = llm.invoke(explain_prompt).content.strip()

                    st.markdown(f"## ğŸ’¡ {topic_keyword}ë€ ë¬´ì—‡ì¼ê¹Œìš”?")
                    st.write(explanation)

                    # 2ë‹¨ê³„: ì´ êµì¬ì—ì„œëŠ” í•´ë‹¹ ì£¼ì œë¥¼ ì§ì ‘ ë‹¤ë£¨ì§€ ì•ŠìŒì„ ì•Œë¦¼
                    st.markdown("---")
                    st.warning(f"ğŸ“­ ì´ êµì¬ì—ì„œëŠ” '{topic_keyword}'ì„(ë¥¼) ì§ì ‘ ë‹¤ë£¨ëŠ” ê°•ì˜ëŠ” ì—†ì–´ìš”.")

                    # 3ë‹¨ê³„: ê´€ë ¨ ì£¼ì œ ì¶”ì²œ (LLMì´ ë¬¸ë²•ì ìœ¼ë¡œ ì—°ê´€ëœ ì£¼ì œë“¤ ë‚˜ì—´)
                    related_prompt = f"""'{topic_keyword}'ê³¼ ê´€ë ¨ëœ ì˜ì–´ ë¬¸ë²• ì£¼ì œ 5ê°œë¥¼ ë‚˜ì—´í•˜ì„¸ìš”.

ê·œì¹™:
- ì˜ì–´ ë¬¸ë²•/íšŒí™” ê´€ë ¨ ì£¼ì œë§Œ (í”„ë¡œê·¸ë˜ë° ê¸ˆì§€)
- ì˜¤ì§ ì£¼ì œ ì´ë¦„ë§Œ ì¶œë ¥ (ì„¤ëª…, ë²ˆí˜¸, ë¬¸ì¥ ê¸ˆì§€)
- ì½¤ë§ˆë¡œ êµ¬ë¶„
- ì˜ˆì‹œ ì¶œë ¥: í˜„ì¬ì§„í–‰í˜•, í˜„ì¬ì™„ë£Œ, ê³¼ê±°ì‹œì œ, beë™ì‚¬, ì¡°ë™ì‚¬

ì¶œë ¥:"""
                    related_result = llm.invoke(related_prompt).content.strip()

                    if related_result:
                        related_topics = [t.strip() for t in related_result.split(",")]
                        available_topics = list(knowledge_graph.keys())

                        st.markdown("### ğŸ”— ê´€ë ¨ ì£¼ì œ")

                        for topic in related_topics:
                            # ì§€ì‹ ê·¸ë˜í”„ì—ì„œ ë§¤ì¹­ í™•ì¸
                            matched_kg_topic = None
                            for kg_topic in available_topics:
                                if topic.lower() in kg_topic.lower() or kg_topic.lower() in topic.lower():
                                    matched_kg_topic = kg_topic
                                    break

                            if matched_kg_topic:
                                # êµì¬ì— ìˆìŒ â†’ RAGë¡œ ì˜ìƒ êµ¬ê°„ ì°¾ê¸°
                                with st.expander(f"âœ… **{topic}** - ê°•ì˜ ìˆìŒ", expanded=False):
                                    rel_docs = retriever.invoke(matched_kg_topic)
                                    if rel_docs:
                                        rel_best = rel_docs[0]
                                        url = rel_best.metadata.get('video_url', '')
                                        start = int(float(rel_best.metadata.get('start_time', 0)))
                                        end = int(float(rel_best.metadata.get('end_time', 0)))

                                        if url:
                                            if "watch?v=" in url:
                                                video_id = url.split("watch?v=")[-1].split("&")[0]
                                            elif "youtu.be/" in url:
                                                video_id = url.split("youtu.be/")[-1].split("?")[0]
                                            else:
                                                video_id = url

                                            embed_url = f"https://www.youtube.com/embed/{video_id}?start={start}&end={end}"
                                            st.components.v1.iframe(embed_url, width=700, height=400)
                            else:
                                # êµì¬ì— ì—†ìŒ
                                st.markdown(f"âŒ **{topic}** - í•´ë‹¹ ê°•ì˜ ì—†ìŒ")            
              
                #st.session_state["context"] = context
            #else:
            #    st.warning("âš ï¸ í•´ë‹¹ ë‚´ìš©ì€ ê°•ì˜ ìë£Œì— ì—†ìŠµë‹ˆë‹¤.")
            #    with st.expander("ğŸ“š í•™ìŠµ ê°€ëŠ¥í•œ ì£¼ì œ", expanded=True):
            #        st.markdown(AVAILABLE_TOPICS)
                            #st.link_button(
                            #    "â–¶ï¸ ì¬ìƒ", 
                            #    f"{url}&t={start}s"
                            #)
                        
            # ============================================================
            # Quiz ëª¨ë“œ
            # ============================================================
#            elif st.session_state["mode"] == "quiz":
#                history = get_session_history(session_id)
#                past_text = "\n".join([m.content for m in history.messages if m.type == "human"])
#                num_questions = 5
#                context = st.session_state.get("context", "")
#                quiz_prompt = ChatPromptTemplate.from_messages([
#                        ("system", "ë„ˆëŠ” ì¹œì ˆí•œ ì˜ì–´ ì„ ìƒë‹˜ì´ì•¼."),
#                        ("human", """ì§€ê¸ˆê¹Œì§€ ì‚¬ìš©ìê°€ í•™ìŠµ ì¤‘ ë¬¼ì–´ë³¸ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ì•„:
#                    {past_text}
#
#                    ì´ ë‚´ìš©ì„ ì°¸ê³ í•´ì„œ ì•„ë˜ {num_questions}ê°œì˜ ê°ê´€ì‹ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì¤˜ ë¬¸ì œëŠ” í•œê¸€ë¡œ.
#
#                    {context}
#
#                    ì¶œë ¥ í˜•ì‹(JSON):
#                    {{
#                        "questions": [
#                            {{
#                                "question": "ë¬¸ì œ",
#                                "options": ["1. ë‹µ1", "2. ë‹µ2", "3. ë‹µ3", "4. ë‹µ4"],
#                                "answer": 1
#                            }}
#                        ]
#                    }}
#                    """)
#                    ])
#                
#                quiz_chain = quiz_prompt | llm
#                quiz = quiz_chain.invoke({
#                    "past_text": past_text,  
#                    "context": context,  
#                    "num_questions": 5    
#                })
#
#                import re
#                def safe_json_parse(text: str):
#                    """LangChain ì‘ë‹µì—ì„œ JSON ë³¸ë¬¸ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
#                    if not text or not text.strip():
#                        raise ValueError("ë¹ˆ ì‘ë‹µì…ë‹ˆë‹¤.")
#                    # ì½”ë“œíœìŠ¤ ì œê±°
#                    if text.startswith("```"):
#                        text = re.sub(r"^```(?:json)?", "", text)
#                        text = re.sub(r"```$", "", text)
#                    # JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
#                    m = re.search(r"\{[\s\S]*\}", text)
#                    if not m:
#                        raise ValueError("JSON ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
#                    return json.loads(m.group(0))
#                
#                quiz_json = safe_json_parse(getattr(quiz, "content", ""))
#                            
#                st.title("ğŸ§© ì˜ì–´ í€´ì¦ˆ")
#
#                # ì„¸ì…˜ì— ì‚¬ìš©ì ë‹µì•ˆ ì €ì¥
#                if "user_answers" not in st.session_state:
#                    st.session_state.user_answers = {}
#
#                # ë¬¸ì œ ë Œë”ë§
#                for i, q in enumerate(quiz_json["questions"], 1):
#                    st.markdown(f"**Q{i}. {q['question']}**")
#                    selected = st.radio(
#                        label="",
#                        options=[f"{j+1}. {opt}" for j, opt in enumerate(q["options"])],
#                        key=f"q{i}"
#                    )
#                    st.session_state.user_answers[i] = selected
#            
#            # ============================================================
#            # Review ëª¨ë“œ
#            # ============================================================
#            elif st.session_state["mode"] == "review":
#                st.markdown("ğŸ“– **ë³µìŠµ ìë£Œ**")
# 