"""
ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸ v3.1 - Topic Routing + GPU Reranker
- KG (26) + Smart Search (ì „ì²´)
- 2-stage retrieval: í† í”½ í™•ì • ì‹œ topic filterë¡œ êµ¬ê°„ë§Œ íƒìƒ‰
- CrossEncoder reranker (GPU)ë¡œ "ì •í™•í•œ ì§€ì " ì„ íƒ ê°•í™”
- Chroma scoreëŠ” dist(ê±°ë¦¬)ë¡œ ì·¨ê¸‰
"""

import streamlit as st
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI
from openai import OpenAI
from dotenv import load_dotenv
from rapidfuzz import process, fuzz
import unicodedata
import json
import os
import re
from typing import Optional, List, Tuple, Dict, Any

load_dotenv()

# =========================
# Config
# =========================
OPENAI_REWRITE_MODEL = os.getenv("OPENAI_REWRITE_MODEL", "gpt-4o")
OPENAI_JUDGE_MODEL   = os.getenv("OPENAI_JUDGE_MODEL", "gpt-4o")
OPENAI_EXPLAIN_MODEL = os.getenv("OPENAI_EXPLAIN_MODEL", "gpt-4o")

USE_RERANKER = os.getenv("USE_RERANKER", "1") == "1"
RERANKER_MODEL = os.getenv("RERANKER_MODEL", "BAAI/bge-reranker-v2-m3")

# retrieve í›„ë³´ ìˆ˜: UI k_value(10)ë¼ë©´ 2x=20 ì •ë„ë§Œ rerank
RETRIEVE_MULTIPLIER = float(os.getenv("RETRIEVE_MULTIPLIER", "2.0"))
RETRIEVE_MAX = int(os.getenv("RETRIEVE_MAX", "30"))

# E5 prefix/normalize ì˜µì…˜ (ê¸°ì¡´ DBê°€ prefix ì—†ì´ êµ¬ì¶•ëìœ¼ë©´, ìµœì  ì„±ëŠ¥ ìœ„í•´ ì¬ìƒ‰ì¸ ê¶Œì¥)
USE_E5_PREFIX = os.getenv("USE_E5_PREFIX", "0") == "1"

st.set_page_config(page_title="ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸ v3.1", page_icon="ğŸ“š", layout="wide")

# =========================
# Utils
# =========================
def normalize(s: str) -> str:
    return unicodedata.normalize("NFC", s.lower().replace(" ", ""))

def safe_float(x, default=0.0) -> float:
    try:
        if x is None:
            return default
        return float(x)
    except Exception:
        return default

def is_basic_definition_question(q: str) -> bool:
    # "~ê°€ ë­ì•¼", "ëœ»", "ì •ì˜", "what is", "meaning" ë¥˜
    qn = q.strip().lower()
    patterns = [
        r"ë­ì•¼\??$", r"ë¬´ìŠ¨ëœ»\??$", r"ëœ»ì´ ë­ì•¼\??$", r"ì •ì˜\??$", r"ì„¤ëª…í•´ì¤˜\??$",
        r"what is\b", r"meaning\b", r"define\b",
    ]
    return any(re.search(p, qn) for p in patterns)

def fuzzy_match_topic(query: str, topic_list: List[str], threshold: int = 85) -> Optional[str]:
    q = normalize(query)
    candidates = [normalize(t) for t in topic_list]
    mr = process.extractOne(q, candidates, scorer=fuzz.ratio)
    if not mr:
        return None
    _match, score, idx = mr
    if score >= threshold:
        return topic_list[idx]
    return None

# =========================
# Vectorstore (Chroma) load
# =========================
class E5Embeddings(HuggingFaceEmbeddings):
    def embed_query(self, text: str):
        if USE_E5_PREFIX:
            text = "query: " + text
        return super().embed_query(text)

    def embed_documents(self, texts):
        if USE_E5_PREFIX:
            texts = ["passage: " + t for t in texts]
        return super().embed_documents(texts)

@st.cache_resource
def load_vectorstore():
    embeddings = E5Embeddings(
        model_name="intfloat/multilingual-e5-large",
        encode_kwargs={"normalize_embeddings": True} if USE_E5_PREFIX else {},
    )
    vectorstore = Chroma(
        persist_directory="/data/edutem/sooine/rag_bot/chroma_db",
        embedding_function=embeddings,
    )
    return vectorstore

vectorstore = load_vectorstore()

@st.cache_resource
def load_all_topics_from_chroma() -> List[str]:
    # small corpus(384 docs)ì´ë©´ ì¶©ë¶„íˆ ê°ë‹¹ë¨
    metas = vectorstore._collection.get(include=["metadatas"])["metadatas"]
    topics = []
    for m in metas:
        if not m:
            continue
        t = m.get("topic")
        if t:
            topics.append(t)
    # unique, stable order
    seen = set()
    uniq = []
    for t in topics:
        if t not in seen:
            seen.add(t)
            uniq.append(t)
    return uniq

ALL_TOPICS = load_all_topics_from_chroma()

client = OpenAI()
llm = ChatOpenAI(model=OPENAI_EXPLAIN_MODEL, temperature=0)

# =========================
# KG load
# =========================
@st.cache_resource
def load_knowledge_graph(_mtime):
    with open('/data/edutem/sooine/rag_bot/knowledge_graph.json', 'r', encoding='utf-8') as f:
        return json.load(f)

kg_path = '/data/edutem/sooine/rag_bot/knowledge_graph.json'
kg_mtime = os.path.getmtime(kg_path)
knowledge_graph = load_knowledge_graph(kg_mtime)

def search_in_knowledge_graph(query: str) -> Optional[Dict[str, Any]]:
    query_lower = query.lower().strip()
    topic_list = list(knowledge_graph.keys())

    # exact
    for main_topic in topic_list:
        if main_topic.lower() == query_lower:
            return {"type": "main_topic", "main_topic": main_topic, "data": knowledge_graph[main_topic]}

    # substring
    for main_topic in topic_list:
        if query_lower in main_topic.lower() or main_topic.lower() in query_lower:
            return {"type": "main_topic", "main_topic": main_topic, "data": knowledge_graph[main_topic]}

    # fuzzy
    best = fuzzy_match_topic(query, topic_list, threshold=80)
    if best:
        return {"type": "main_topic", "main_topic": best, "data": knowledge_graph[best]}

    return None

# =========================
# Query rewrite
# =========================
@st.cache_data(show_spinner=False)
def rewrite_query(query: str) -> str:
    response = client.chat.completions.create(
        model=OPENAI_REWRITE_MODEL,
        messages=[{
            "role": "user",
            "content": f"""ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ì˜ì–´ ë¬¸ë²• í† í”½ì„ ì¶”ì¶œí•˜ì„¸ìš”.

ì‚¬ìš©ì ì§ˆë¬¸: {query}

ê·œì¹™:
- í•µì‹¬ ë¬¸ë²• ìš©ì–´ë§Œ ì¶”ì¶œ
- ì˜¤íƒ€ êµì • (ë¹„ë™ì‚¬ â†’ beë™ì‚¬, ë¨¸ì•¼ â†’ ë­ì•¼)
- ì§ˆë¬¸ í˜•ì‹ ì œê±°
- ì˜ˆ: "ë¹„ë™ì‚¬ê°€ ë¨¸ì•¼?" â†’ "beë™ì‚¬"
- ì˜ˆ: "have pp ì–´ë–»ê²Œ ì¨?" â†’ "í˜„ì¬ì™„ë£Œ"

ì¶œë ¥: í† í”½ë§Œ (ì„¤ëª… ì—†ì´)"""
        }],
        max_tokens=50,
        temperature=0
    )
    return response.choices[0].message.content.strip()

# =========================
# Reranker (GPU)
# =========================
@st.cache_resource
def load_reranker():
    if not USE_RERANKER:
        return None
    try:
        import torch
        from sentence_transformers import CrossEncoder
        device = "cuda" if torch.cuda.is_available() else "cpu"
        # max_length ì¤„ì´ë©´ ì†ë„ ì•ˆì •í™”
        reranker = CrossEncoder(RERANKER_MODEL, device=device, max_length=512)
        return reranker
    except Exception as e:
        # sentence-transformers ë¯¸ì„¤ì¹˜ ë“±
        return None

RERANKER = load_reranker()

def rerank(query: str, docs_with_dist: List[Tuple[Any, float]]) -> List[Tuple[Any, float, float]]:
    """
    Args:
      docs_with_dist: [(doc, dist), ...]
    Returns:
      [(doc, dist, rerank_score), ...] sorted by rerank_score desc
    """
    if not RERANKER or not docs_with_dist:
        return [(d, dist, 0.0) for d, dist in docs_with_dist]

    pairs = [(query, d.page_content) for d, _dist in docs_with_dist]
    scores = RERANKER.predict(pairs)
    packed = [(docs_with_dist[i][0], docs_with_dist[i][1], float(scores[i])) for i in range(len(docs_with_dist))]
    packed.sort(key=lambda x: x[2], reverse=True)
    return packed

# =========================
# Context formatting for judge
# =========================
def format_results_with_metadata(results: List[Tuple[Any, float, float]]) -> str:
    """
    results: [(doc, dist, rerank_score), ...]
    """
    if not results:
        return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

    parts = []
    for i, (doc, dist, rr) in enumerate(results):
        topic = doc.metadata.get('topic', 'ì•Œ ìˆ˜ ì—†ìŒ')
        start_time = safe_float(doc.metadata.get('start_time', 0))
        end_time = safe_float(doc.metadata.get('end_time', 0))
        video_url = doc.metadata.get('video_url', '')

        start_str = f"{int(start_time//60)}:{int(start_time%60):02d}"
        end_str = f"{int(end_time//60)}:{int(end_time%60):02d}"
        duration = max(0.0, end_time - start_time)

        content = (doc.page_content or "").replace("\n", " ")
        content_preview = content[:240] + ("..." if len(content) > 240 else "")

        parts.append(
            f"""[í›„ë³´ {i}]
- í† í”½: {topic}
- ì‹œê°„: {start_str} ~ {end_str} ({duration:.0f}ì´ˆ)
- dist(ì‘ì„ìˆ˜ë¡ ìœ ì‚¬): {dist:.4f}
- rerank(í´ìˆ˜ë¡ ì í•©): {rr:.4f}
- ë‚´ìš©: {content_preview}"""
        )
    return "\n\n".join(parts)

def build_alternative_topics(results: List[Tuple[Any, float, float]], max_n: int = 3) -> List[str]:
    seen = set()
    alts = []
    for doc, _dist, _rr in results:
        t = doc.metadata.get("topic", "")
        if t and t not in seen:
            seen.add(t)
            alts.append(t)
        if len(alts) >= max_n:
            break
    return alts

# =========================
# YouTube embed
# =========================
def get_video_embed(url: str, start: float, end: float) -> Optional[str]:
    if not url:
        return None
    if "watch?v=" in url:
        video_id = url.split("watch?v=")[-1].split("&")[0]
    elif "youtu.be/" in url:
        video_id = url.split("youtu.be/")[-1].split("?")[0]
    else:
        return None
    return f"https://www.youtube.com/embed/{video_id}?start={int(start)}&end={int(end)}"

# =========================
# Topic routing + search
# =========================
def resolve_topic(rewritten: str) -> Optional[str]:
    # exact ë¨¼ì €
    if rewritten in ALL_TOPICS:
        return rewritten
    # fuzzyë¡œ í† í”½ ì •ê·œí™”
    best = fuzzy_match_topic(rewritten, ALL_TOPICS, threshold=88)
    return best

def retrieve_candidates(query: str, rewritten: str, k_ui: int, pinned_topic: Optional[str] = None):
    k_retrieve = min(int(k_ui * RETRIEVE_MULTIPLIER), RETRIEVE_MAX)

    # 1) í† í”½ì´ pinë˜ì—ˆê±°ë‚˜ resolveë˜ë©´: topic filter ê²€ìƒ‰ (êµ¬ê°„ ì •ë°€)
    topic = pinned_topic or resolve_topic(rewritten)
    if topic:
        # í† í”½ ë‚´ë¶€ì—ì„œ "ì› ì§ˆë¬¸"ìœ¼ë¡œ êµ¬ê°„ì„ ì°¾ëŠ” ê²Œ ë” ì˜ ë§ìŒ
        raw = vectorstore.similarity_search_with_score(query, k=k_retrieve, filter={"topic": topic})
        return topic, raw

    # 2) í† í”½ í™•ì • ë¶ˆê°€: ì „ì—­ ê²€ìƒ‰ì€ rewrittenìœ¼ë¡œ (ë¬¸ë²• ìš©ì–´ ì¤‘ì‹¬)
    raw = vectorstore.similarity_search_with_score(rewritten, k=k_retrieve)
    return None, raw

def judge_best(query: str, rewritten: str, candidates: List[Tuple[Any, float, float]], k_ui: int) -> Dict[str, Any]:
    """
    candidates: reranked list [(doc, dist, rr), ...] already sorted by rr desc
    """
    if not candidates:
        return {
            "confidence": "low",
            "found": False,
            "best_index": 0,
            "best_topic": "",
            "reasoning": "ê²€ìƒ‰ í›„ë³´ê°€ ì—†ì–´ì„œ ê´€ë ¨ ì˜ìƒì„ ì„ íƒí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "alternative_topics": []
        }

    context = format_results_with_metadata(candidates[:k_ui])

    response = client.chat.completions.create(
        model=OPENAI_JUDGE_MODEL,
        messages=[
            {
                "role": "system",
                "content": """ë‹¹ì‹ ì€ ì˜ì–´ ë¬¸ë²• êµìœ¡ ì˜ìƒ ê²€ìƒ‰ ì‹œìŠ¤í…œì˜ íŒë‹¨ìì…ë‹ˆë‹¤.
í›„ë³´ êµ¬ê°„ë“¤ì„ ë³´ê³  ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì í•©í•œ êµ¬ê°„ì„ ì„ íƒí•˜ì„¸ìš”.

ì¤‘ìš”:
- distëŠ” ê±°ë¦¬(distance)ì´ë©°, ì‘ì„ìˆ˜ë¡ ìœ ì‚¬í•©ë‹ˆë‹¤.
- rerank ì ìˆ˜ëŠ” í´ìˆ˜ë¡ ì§ˆë¬¸ ì í•©ë„ê°€ ë†’ìŠµë‹ˆë‹¤.
- ì ìˆ˜ëŠ” ì°¸ê³ ìš©ì´ê³ , í† í”½/ë‚´ìš© ì í•©ë„ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë´…ë‹ˆë‹¤.

íŒë‹¨:
1) high: ì§ˆë¬¸ í† í”½ê³¼ í›„ë³´ í† í”½/ë‚´ìš©ì´ ì§ì ‘ì ìœ¼ë¡œ ì •í™•íˆ ì¼ì¹˜
2) medium: ì§ì ‘ ì¼ì¹˜ëŠ” ì•„ë‹ˆì§€ë§Œ ì¶©ë¶„íˆ ë‹µë³€ ê°€ëŠ¥í•œ ì¸ì ‘ í† í”½
3) low: ê´€ë ¨ì„±ì´ ë‚®ìŒ
"""
            },
            {
                "role": "user",
                "content": f"""
ì§ˆë¬¸: {query}
ê²€ìƒ‰ì–´(í† í”½ ì¶”ì¶œ): {rewritten}

í›„ë³´:
{context}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€:
{{
  "confidence": "high" | "medium" | "low",
  "found": true | false,
  "best_index": 0ë¶€í„° {min(k_ui-1, len(candidates)-1)} ì‚¬ì´ ì •ìˆ˜,
  "best_topic": "ì„ íƒí•œ í† í”½ëª…",
  "reasoning": "ì„ íƒ ì´ìœ  (í•œêµ­ì–´ 2-3ë¬¸ì¥)",
  "alternative_topics": ["ë‹¤ë¥¸ í›„ë³´ í† í”½ (ìµœëŒ€ 3ê°œ)"]
}}
"""
            }
        ],
        response_format={"type": "json_object"},
        temperature=0
    )
    try:
        out = json.loads(response.choices[0].message.content)
    except Exception:
        out = {
            "confidence": "medium",
            "found": True,
            "best_index": 0,
            "best_topic": candidates[0][0].metadata.get("topic", ""),
            "reasoning": "íŒë‹¨ JSON íŒŒì‹±ì— ì‹¤íŒ¨í•˜ì—¬ 1ìˆœìœ„ í›„ë³´ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.",
            "alternative_topics": build_alternative_topics(candidates[1:], 3)
        }

    # safety
    bi = out.get("best_index", 0)
    if bi is None or not isinstance(bi, int) or bi < 0 or bi >= len(candidates):
        out["best_index"] = 0

    if out.get("confidence") == "low":
        out["found"] = False

    if not out.get("alternative_topics"):
        out["alternative_topics"] = build_alternative_topics(candidates[1:], 3)

    return out

def smart_search_v3(query: str, k_ui: int = 10, rewritten: Optional[str] = None, pinned_topic: Optional[str] = None) -> Dict[str, Any]:
    """
    Returns:
      dict with fields:
      - confidence, found, best_topic, reasoning, alternative_topics
      - rewritten, dist, rerank_score, doc, video_url, start_time, end_time
      - candidates (reranked)
    """
    if rewritten is None:
        rewritten = rewrite_query(query)

    # retrieve
    resolved_topic, raw = retrieve_candidates(query, rewritten, k_ui, pinned_topic=pinned_topic)

    if not raw:
        return {
            "found": False,
            "confidence": "low",
            "rewritten": rewritten,
            "message": "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.",
            "candidates": []
        }

    # raw: [(doc, dist), ...]
    # rerank: í›„ë³´ ìˆ˜ëŠ” k_ui*2 (ìµœëŒ€ 30) ìˆ˜ì¤€ì—ì„œë§Œ
    reranked = rerank(query, raw)

    # 2-stageë¡œ í† í”½ì´ í™•ì •ëœ ê²½ìš°ëŠ” ê¸°ë³¸ì ìœ¼ë¡œ highë¡œ ì²˜ë¦¬ ê°€ëŠ¥(íŠ¹íˆ ì •ì˜í˜• ì§ˆë¬¸)
    if resolved_topic and is_basic_definition_question(query):
        best_doc, best_dist, best_rr = reranked[0]
        return {
            "confidence": "high",
            "found": True,
            "best_index": 0,
            "best_topic": resolved_topic,
            "reasoning": "ì§ˆë¬¸ì´ ê¸°ë³¸ ê°œë…(ì •ì˜/ì˜ë¯¸) ìœ í˜•ì´ë©°, í† í”½ì´ DBì— ì¡´ì¬í•´ í•´ë‹¹ í† í”½ ë‚´ë¶€ êµ¬ê°„ì—ì„œ ìµœì  í›„ë³´ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.",
            "alternative_topics": build_alternative_topics(reranked[1:], 3),
            "rewritten": rewritten,
            "dist": best_dist,
            "rerank_score": best_rr,
            "doc": best_doc,
            "video_url": best_doc.metadata.get("video_url"),
            "start_time": safe_float(best_doc.metadata.get("start_time", 0)),
            "end_time": safe_float(best_doc.metadata.get("end_time", 0)),
            "candidates": reranked
        }

    # ê·¸ ì™¸ëŠ” judgeë¡œ confidence/ì„ íƒ index ê²°ì •
    judgment = judge_best(query, rewritten, reranked, k_ui)

    best_idx = judgment.get("best_index", 0)
    best_doc, best_dist, best_rr = reranked[best_idx]
    best_topic = judgment.get("best_topic") or best_doc.metadata.get("topic", "")

    return {
        **judgment,
        "rewritten": rewritten,
        "dist": best_dist,
        "rerank_score": best_rr,
        "doc": best_doc,
        "video_url": best_doc.metadata.get("video_url"),
        "start_time": safe_float(best_doc.metadata.get("start_time", 0)),
        "end_time": safe_float(best_doc.metadata.get("end_time", 0)),
        "best_topic": best_topic,
        "candidates": reranked
    }

# =========================
# UI
# =========================
st.title("ğŸ“š ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸ v3.1")
st.markdown("**ê°œì„ **: Topic Routing + GPU Reranker (retrieveëŠ” 10~20ê°œë§Œ, ê·¸ ì•ˆì—ì„œë§Œ rerank)")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")

    search_mode = st.radio(
        "ê²€ìƒ‰ ëª¨ë“œ",
        ["ğŸ” í•˜ì´ë¸Œë¦¬ë“œ (KG ìš°ì„ )", "âš¡ Smart Searchë§Œ"],
        index=0,
        help="í•˜ì´ë¸Œë¦¬ë“œ: KGì—ì„œ ë¨¼ì € ì°¾ê³  ì—†ìœ¼ë©´ Smart Search\nSmart Searchë§Œ: ì „ì²´ í† í”½ì—ì„œ ì§ì ‘ ê²€ìƒ‰"
    )

    st.divider()

    k_value = st.slider(
        "í‘œì‹œ/íŒë‹¨ í›„ë³´ ìˆ˜ (k)",
        min_value=5,
        max_value=20,
        value=10,
        step=1,
        help="ìµœì¢… í›„ë³´ í‘œì‹œ/íŒë‹¨ì— ì“°ëŠ” k. ë‚´ë¶€ retrieveëŠ” k*2 (ìµœëŒ€ 30)ë¡œë§Œ í™•ì¥ë¨."
    )

    st.divider()

    st.markdown("### ğŸ“š KG ì£¼ì œ")
    st.caption(f"{len(knowledge_graph)}ê°œ")
    with st.expander("ì£¼ì œ ëª©ë¡ ë³´ê¸°"):
        for topic in sorted(knowledge_graph.keys()):
            st.markdown(f"- {topic}")

    st.divider()

    st.markdown("### ğŸ“Š ì‹œìŠ¤í…œ")
    st.markdown(f"- Chroma topics: {len(ALL_TOPICS)}")
    st.markdown(f"- KG: {len(knowledge_graph)}")
    st.markdown(f"- Rewrite/Judge: {OPENAI_REWRITE_MODEL} / {OPENAI_JUDGE_MODEL}")
    st.markdown(f"- Explain: {OPENAI_EXPLAIN_MODEL}")
    st.markdown(f"- Reranker: {'ON' if RERANKER else 'OFF'} ({RERANKER_MODEL})")
    st.markdown(f"- E5 prefix: {'ON' if USE_E5_PREFIX else 'OFF'}")
    st.markdown("- ë²„ì „: v3.1")

    st.divider()
    if st.button("ğŸ—‘ï¸ ëŒ€í™” ì´ˆê¸°í™”"):
        st.session_state["messages"] = []
        st.rerun()

user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")

if user_input:
    with st.chat_message("user"):
        st.markdown(user_input)

    with st.chat_message("assistant"):
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):

            if "í•˜ì´ë¸Œë¦¬ë“œ" in search_mode:
                rewritten = rewrite_query(user_input)
                kg_result = search_in_knowledge_graph(rewritten)

                if kg_result:
                    st.caption("ğŸ” Knowledge Graphì—ì„œ ì°¾ìŒ")
                    main_topic = kg_result["main_topic"]

                    st.markdown(f"## ğŸ’¡ {main_topic}")
                    with st.spinner("ì„¤ëª… ìƒì„± ì¤‘..."):
                        explain_prompt = f"'{main_topic}'ì´ ë¬´ì—‡ì¸ì§€ í•µì‹¬ë§Œ ê°„ë‹¨íˆ 2-3ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”."
                        explanation = llm.invoke(explain_prompt).content.strip()
                    st.write(explanation)

                    st.markdown("---")
                    st.success(f"âœ… '{main_topic}' ê°•ì˜ê°€ ìˆì–´ìš”!")

                    # í† í”½ í™•ì •: pinned_topicìœ¼ë¡œ topic-filter êµ¬ê°„ ê²€ìƒ‰
                    result = smart_search_v3(user_input, k_ui=k_value, rewritten=main_topic, pinned_topic=main_topic)

                else:
                    st.caption("âš¡ KGì— ì—†ìŒ â†’ Smart Search ì‚¬ìš©")
                    result = smart_search_v3(user_input, k_ui=k_value, rewritten=rewritten)

            else:
                result = smart_search_v3(user_input, k_ui=k_value)

            # ===== Render =====
            keyword = result.get("best_topic") or user_input
            rewritten_q = result.get("rewritten", "")
            confidence = result.get("confidence", "low")
            dist = result.get("dist", 0.0)
            rr = result.get("rerank_score", 0.0)

            st.markdown(f"## ğŸ’¡ {keyword}")
            st.caption(f"ğŸ” {user_input} â†’ {rewritten_q} â†’ {keyword}")

            if confidence == "high":
                st.success(f"âœ… '{keyword}' ê°•ì˜ë¥¼ ì°¾ì•˜ì–´ìš”!")
            elif confidence == "medium":
                st.info(f"ğŸŸ¡ '{keyword}' í† í”½ì´ ê´€ë ¨ìˆì„ ìˆ˜ ìˆì–´ìš”")
            else:
                st.warning(f"ğŸ“­ '{keyword}'ì„(ë¥¼) ì§ì ‘ ë‹¤ë£¨ëŠ” ê°•ì˜ëŠ” ì—†ì–´ìš”")

            st.caption(f"ğŸ¯ í™•ì‹ ë„: {confidence} | dist: {dist:.4f} | rerank: {rr:.4f}")

            if result.get("reasoning"):
                with st.expander("ğŸ’¡ íŒë‹¨ ê·¼ê±°"):
                    st.write(result["reasoning"])

            if confidence in ["high", "medium"]:
                video_url = result.get("video_url")
                start = safe_float(result.get("start_time", 0))
                end = safe_float(result.get("end_time", 0))

                if video_url:
                    st.markdown("### ğŸ“º ê°•ì˜ ì˜ìƒ")
                    embed_url = get_video_embed(video_url, start, end)
                    if embed_url:
                        st.components.v1.iframe(embed_url, width=800, height=450)
                        st.caption(f"â±ï¸ {int(start)}ì´ˆ ~ {int(end)}ì´ˆ")

                st.markdown("---")
                st.markdown("### ğŸ”— ê´€ë ¨ ì£¼ì œ (rerank ìƒìœ„ í›„ë³´)")
                shown = set()
                for i, (doc, d, rscore) in enumerate(result.get("candidates", [])[:6]):
                    t = doc.metadata.get("topic", "")
                    if not t or t in shown:
                        continue
                    shown.add(t)
                    with st.expander(f"âœ… {t} (dist: {d:.3f}, rerank: {rscore:.3f})"):
                        rel_url = doc.metadata.get("video_url", "")
                        rel_start = safe_float(doc.metadata.get("start_time", 0))
                        rel_end = safe_float(doc.metadata.get("end_time", 0))
                        rel_embed = get_video_embed(rel_url, rel_start, rel_end)
                        if rel_embed:
                            st.components.v1.iframe(rel_embed, width=700, height=400)

            if confidence == "low" and result.get("alternative_topics"):
                st.markdown("### ğŸ” ì´ëŸ° í† í”½ë“¤ì„ ì°¾ì•„ë³´ì‹œê² ì–´ìš”?")
                for alt in result["alternative_topics"]:
                    st.markdown(f"- {alt}")
