from pathlib import Path
from dotenv import load_dotenv
from openai import OpenAI
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

import json
import glob
import re
import unicodedata
import difflib

load_dotenv()

client = OpenAI()
video_url_map = {
    "01_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #29 | ê³¼ê±°ì— ìˆì—ˆë˜ ì¼ ì„¤ëª…í•  ë•Œ  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=R_-pgaQYaYQ",
    "02_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #28 | ë§ˆë²•ê³¼ ê°™ì€ that | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=008886a-lQI",
    "03_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° ##27 | ìˆ˜ë™íƒœë¥¼ thatê³¼ ì—°ê²°í•˜ê¸°! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=r3qBF9dMz10",
    "04_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #26 | ìˆ˜ë™íƒœ í•µì‹¬ íŒŒì•…! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=bGl_7acUnNk",
    "05_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #25 | haveë¥¼ ê¹Šì´, ìì—°ìŠ¤ëŸ½ê²Œ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=jOb8mznvX48",
    "06_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #24 | í˜„ì¬ì™„ë£Œ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=RgNbTRRt78Y",
    "07_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #23 | that ë’¤ì— ì¡°ë™ì‚¬ ì“°ê¸°  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=OaTujaboBf8",
    "08_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #22 | that ë’¤ì— ì§„í–‰í˜•ì„ ê°€ì§€ê³  ë§Œë“¤ì–´ ë³´ì  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=NXbGg9nxpdk",
    "09_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #21 | ë˜ ë‹¤ë¥¸ that | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=k3-666q27Ps",
    "10_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #20 | thatìœ¼ë¡œ ë¬¸ì¥ì„ ê¸¸ê²Œ! (3) | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=JMfB_2pfqCA",
    "11_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #19 | that ìœ¼ë¡œ ë¬¸ì¥ ê¸¸ê²Œ ë§Œë“¤ê¸°(2) | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=gHrI6qhbziI",
    "12_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#18 | that ìœ¼ë¡œ ë¬¸ì¥ ê¸¸ê²Œ ë§Œë“¤ê¸°(1) | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=nHbEN7KEmmE",
    "13_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #17 | canì„ be able toë¡œ ë°”ê¾¸ê¸°! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=CGN1TdvhkvY",
    "14_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #16 | ë™ëª…ì‚¬ë¡œ ì£¼ì–´ ê¸¸ê²Œ ë§Œë“¤ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=jzJzdoBdeAc",
    "15_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #15 | ëª©ì ì„ ë‚˜íƒ€ë‚´ëŠ” to ë™ì‚¬ì›í˜• | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=7CvXgPmdD9s",
    "16_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #14 | ë¯¸ë˜ì™€ ê³¼ê±°ì—ë„ ì§„í–‰í˜•ì„ ì“´ë‹¤! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=7ot7hY8wm4Q",
    "17_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #13 | í˜„ì¬ ì§„í–‰í˜•ì˜ ë‹¤ì–‘í•œ ì“°ì„.json": "https://www.youtube.com/watch?v=j87QB9EZZrY",
    "18_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #12 | '~ì— ìˆë‹¤'ë¥¼ ëœ»í•˜ëŠ” be ë™ì‚¬ ì—°ìŠµ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=h_Yv5bX8p8k",
    "19_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#11 | ë‹¤ì–‘í•œ be ë™ì‚¬ í˜•íƒœ ì—°ìŠµ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=WS5hLZV7Lb4",
    "20_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#10 | ë°”ì˜ë‹¤ëŠ” busyê°€ ì•„ë‹ˆë‹¤! | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=V8e_cwY7VTs",
    "21_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#9 | ifë¡œ ë¬¸ì¥ ê¸¸ê²Œ ë§Œë“¤ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=hh9pAAS-gho",
    "22_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#8 | '~í•˜ê¸°ë¥¼' to ë™ì‚¬ì›í˜• ì—°ìŠµ | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=TK1HL_27g6U",
    "23_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #5  | ì˜ë¬¸ë¬¸ ê¸¸ê²Œ ë§Œë“¤ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=EXKS6rZHbbA",
    "24_NEW ì´ì‹œì›ì˜ ê¸°ì´ˆ ì˜ì–´ íšŒí™” ê°•ì˜í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #4 | ê³¼ê±° ì‹œì œ ë§ˆìŠ¤í„° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=G_SNroMhJTQ",
    "25_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#3 | And ë¡œ ë¬¸ì¥ì„ ê¸¸ê²Œ!  | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=VJeidy58uJQ",
    "26_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#1 | ì˜ì–´ëŠ” ë‹¨ì–´ì˜ ì—°ê²° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=oLIpoVoDgTo",
    "27_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#2 | ë¯¸ë˜ë¥¼ ë‚˜íƒ€ë‚´ëŠ” will | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=KQbWy6j_TFA",
    "28_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #30 | ë§ì´ ì“°ëŠ” ë™ì‚¬ put / get / take | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=1dIALFMvJlA",
    "29_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸°#6 | ì˜ë¬¸ë¬¸ ì§ˆë¬¸ì— ë‹µí•˜ê¸° | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=a_eNZ4ZVwxc",
    "30_NEW ì´ì‹œì› ê°•ì˜ | í•œ ë‹¬ ë§Œì— ì˜ì–´ë¡œ ë§ë¬¸ íŠ¸ê¸° #7 | ê°€ëŠ¥ê³¼ í—ˆë½ì˜ can | ê¸°ì´ˆ ì˜ì–´ íšŒí™”.json": "https://www.youtube.com/watch?v=kYx8f4U4-jo"
}


def normalize_filename(name):
    """ë§¥OS/ë¦¬ëˆ…ìŠ¤ í˜¸í™˜ì„ ìœ„í•œ íŒŒì¼ëª… ì •ê·œí™”"""
    name = unicodedata.normalize("NFC", name)
    name = name.replace("ï½œ", "|").replace("ï¼ƒ", "#")
    name = re.sub(r"\s+", " ", name.strip())
    return name


def get_video_url(filename):
    """
    Whisperë¡œ ìƒì„±ëœ JSON íŒŒì¼ëª…ê³¼ video_url_map í‚¤ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ.
    fuzzy matchingìœ¼ë¡œ ê°€ì¥ ë¹„ìŠ·í•œ keyë¥¼ ì°¾ì•„ URL ë§¤í•‘.
    """
    normalized_keys = list(video_url_map.keys())
    match = difflib.get_close_matches(filename, normalized_keys, n=1, cutoff=0.6)
    if match:
        return video_url_map[match[0]]
    return "URL_ì—†ìŒ"

# Step 1: ì˜¤ì¸ì‹ íŒ¨í„´ ì¶”ì¶œ (LLM ì‚¬ìš©)
# ============================================================
def extract_corrections_from_transcript(transcript_text: str) -> dict:
    all_corrections = {}
    
    # 1500ìì”© ì²­í¬ë¡œ ë‚˜ëˆ”
    chunk_size = 1500
    for i in range(0, len(transcript_text), chunk_size):
        chunk = transcript_text[i:i + chunk_size]
        prompt = f"""ë‹¹ì‹ ì€ ì˜ì–´ ê°•ì˜ ìŒì„±ì¸ì‹ ì˜¤ë¥˜ë¥¼ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” í•œêµ­ì¸ ì˜ì–´ ì„ ìƒë‹˜ì˜ ê°•ì˜ë¥¼ Whisperë¡œ ìŒì„±ì¸ì‹í•œ ê²°ê³¼ì…ë‹ˆë‹¤.
ì„ ìƒë‹˜ì´ ì˜ì–´ ë‹¨ì–´ë¥¼ ë°œìŒí–ˆëŠ”ë° í•œê¸€ë¡œ ì˜ëª» ì¸ì‹ëœ ë¶€ë¶„ì„ ì°¾ì•„ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
 - "beë™ì‚¬" â†’ "ë¹„ë™ì‚¬"
 - "have" â†’ "í•´ë¸Œ", "í•´ë¶€"
 - "was" â†’ "ì›Œì¦ˆ"
 - "been" â†’ "ë¹ˆ", "ë¹ˆ"
 
 ì˜ëª»ëœ ì˜ˆì‹œ (ì´ê±´ í•˜ì§€ ë§ˆì„¸ìš”): ì˜ì–´ë‹¨ì–´ë¥¼ ë²ˆì—­ì€ í•˜ì§€ë§ˆì„¸ìš”
- "ì‚¬ëŒë“¤ì´" â†’ "people"  (ì´ê±´ ë²ˆì—­ì„)
- "ì˜¤ëŠ˜" â†’ "today" (ì´ê±´ ë²ˆì—­ì„)

íŠ¸ëœìŠ¤í¬ë¦½íŠ¸:
{transcript_text[:2000]}

ìœ„ í…ìŠ¤íŠ¸ì—ì„œ í•œê¸€ë¡œ ì˜ëª» ì¸ì‹ëœ ì˜ì–´ ë‹¨ì–´ë“¤ì„ ì°¾ì•„ì„œ JSONìœ¼ë¡œ ì¶œë ¥í•˜ì„¸ìš”.
ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

{{"í•œê¸€ì˜¤ì¸ì‹": "ì˜¬ë°”ë¥¸ì˜ì–´", ...}}

ì˜ˆì‹œ: {{"ì›Œí‚¹": "working", "ìŠ¤íŠœë””": "studying", "ë¹„": "be"}}
"""

        try:
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0,
                max_tokens=800
            )
            corrections = json.loads(response.choices[0].message.content)
            all_corrections.update(corrections)  # í•©ì¹¨
        except:
            continue
    
    return all_corrections


# ============================================================
# Step 2: í…ìŠ¤íŠ¸ ë³´ì • ì ìš©
# ============================================================
def apply_corrections(text: str, corrections: dict) -> str:
    """
    ì˜¤ì¸ì‹ íŒ¨í„´ì„ ì˜¬ë°”ë¥¸ ì˜ì–´ë¡œ êµì²´
    ì£¼ì˜: ê¸´ íŒ¨í„´ë¶€í„° ë¨¼ì € êµì²´ (ë¶€ë¶„ ë§¤ì¹­ ë°©ì§€)
    """
    # ê¸´ íŒ¨í„´ë¶€í„° êµì²´ (ì˜ˆ: "ë¹„ ì›Œí‚¹" â†’ "be working" ë¨¼ì €)
    sorted_corrections = sorted(corrections.items(), key=lambda x: len(x[0]), reverse=True)
    
    for wrong, right in sorted_corrections:
        text = text.replace(wrong, right)
    
    return text


def apply_corrections_to_segments(segments: list, corrections: dict) -> list:
    """ê° segmentì˜ textì— ë³´ì • ì ìš©"""
    corrected_segments = []
    for seg in segments:
        corrected_seg = seg.copy()
        corrected_seg["text"] = apply_corrections(seg["text"], corrections)
        corrected_segments.append(corrected_seg)
    return corrected_segments


# ============================================================
# Step 3: LLM Chunking (ì£¼ì œë³„ êµ¬ê°„ ë³‘í•©)
# ============================================================
def chunk_segments_with_llm(segments: list, video_title: str) -> dict:
    transcript_block = ""
    for seg in segments:
        transcript_block += f"[{seg['start']:.1f} ~ {seg['end']:.1f}] {seg['text'].strip()}\n"

    if len(transcript_block) > 8000:
        transcript_block = transcript_block[:8000]

    prompt = f"""
ë‹¹ì‹ ì€ ì˜ì–´ ê°•ì˜ë¥¼ êµ¬ì¡°í™”í•˜ëŠ” êµìœ¡ ì½˜í…ì¸  ë¶„ì„ê°€ì…ë‹ˆë‹¤.

ì•„ë˜ëŠ” '{video_title}'ì˜ Whisper ì „ì‚¬ë³¸ì…ë‹ˆë‹¤.

ğŸ“Œ ê° chunkì— ëŒ€í•´ **êµìœ¡ì  ì—­í• (role)**ê³¼ **ë¬¸ë²• ë¶„ë¥˜(grammar_type)**ë¥¼ íŒë‹¨í•˜ì„¸ìš”.

roleì€ ì•„ë˜ 4ê°œ ì¤‘ í•˜ë‚˜:
- definition   : ê°œë…ì´ ë¬´ì—‡ì¸ì§€ ì„¤ëª…
- usage        : ë¬¸ë²•ì„ ì–´ë–»ê²Œ ì“°ëŠ”ì§€ ì„¤ëª…
- comparison   : ë‹¤ë¥¸ ë¬¸ë²•ê³¼ ë¹„êµ
- practice     : ì—°ìŠµë¬¸ì œ / ì˜ˆë¬¸ í’€ì´

grammar_typeì€ ì•„ë˜ ë¶„ë¥˜ ê¸°ì¤€ì„ ë”°ë¥´ì„¸ìš”:
- ì‹œì œ: ê³¼ê±°/í˜„ì¬/ë¯¸ë˜/ì™„ë£Œ/ì§„í–‰í˜• ë“±
- beë™ì‚¬: am/is/are/was/were ë‹¨ë… ì„¤ëª… (ì¡°ë™ì‚¬ ì•„ë‹˜!)
- ì¡°ë™ì‚¬: can/will/would/should/must/may ë‹¨ë… ì„¤ëª… (beë™ì‚¬ ì œì™¸!)
- ë¬¸ì¥êµ¬ì¡°: thatì ˆ/ê´€ê³„ëŒ€ëª…ì‚¬/ìˆ˜ë™íƒœ/toë¶€ì •ì‚¬/ë™ëª…ì‚¬/ì¡°ë™ì‚¬+beë™ì‚¬ ê²°í•©(can be ë“±) ë“±
- ì „ì¹˜ì‚¬: in/on/at/for/with ë“±
- ë™ì‚¬: ì¼ë°˜ ë™ì‚¬ í™œìš©
- ê¸°íƒ€: ìœ„ì— í•´ë‹¹ ì•ˆ ë¨

âš ï¸ ì¤‘ìš”:
- beë™ì‚¬(am/is/are)ëŠ” ì¡°ë™ì‚¬ê°€ ì•„ë‹˜!
- ì¡°ë™ì‚¬ëŠ” can/will/should ë“±ë§Œ í•´ë‹¹
- ì¡°ë™ì‚¬+beë™ì‚¬ ê²°í•©(can be, will be ë“±)ì€ "ë¬¸ì¥êµ¬ì¡°"ë¡œ ë¶„ë¥˜!

ì¶œë ¥ í˜•ì‹(JSON only):
{{
  "chunks": [
    {{
      "topic": "ì˜ì–´ ë¬¸ë²• ìš©ì–´",
      "role": "definition | usage | comparison | practice",
      "summary": "50ì ì´ë‚´ ìš”ì•½",
      "grammar_type": "ì‹œì œ | beë™ì‚¬ | ì¡°ë™ì‚¬ | ë¬¸ì¥êµ¬ì¡° | ì „ì¹˜ì‚¬ | ë™ì‚¬ | ê¸°íƒ€",
      "start_time": 0.0,
      "end_time": 0.0,
      "content": "200ì ì´ë‚´ í•µì‹¬ ì„¤ëª…",
      "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2"]
    }}
  ]
}}

Whisper ì „ì‚¬ë³¸:
{transcript_block}
"""

    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[{"role": "user", "content": prompt}],
        response_format={"type": "json_object"},
        temperature=0,
        max_tokens=8000
    )
    return json.loads(response.choices[0].message.content)



# ============================================================
# Step 4: Document ìƒì„±
# ============================================================
def build_documents_from_chunks(chunked, video_url, video_filename):
    chunks = chunked.get("chunks", [])
    docs = []

    for chunk in chunks:
        keywords = chunk.get("keywords", [])
        keywords_str = ", ".join(keywords) if isinstance(keywords, list) else ""

        doc = Document(
            page_content=chunk["content"],
            metadata={
                "topic": chunk["topic"],
                "role": chunk["role"],   # âœ… í•µì‹¬ ì¶”ê°€
                "summary": chunk["summary"],
                "grammar_type": chunk.get("grammar_type", ""),
                "start_time": chunk["start_time"],
                "end_time": chunk["end_time"],
                "video_url": video_url,
                "video_filename": video_filename,
                "keywords": keywords_str,
            }
        )
        docs.append(doc)

    return docs


import os
import glob
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma

json_files = sorted(glob.glob("./merged_data/*.json"))

all_docs = []

for json_file in json_files:
    video_filename = normalize_filename(os.path.basename(json_file))
    video_url = get_video_url(video_filename)
    
    with open(json_file, "r", encoding="utf-8") as f:
        result = json.load(f)
    
    segments = result["segments"]
    # STT + OCR í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
    transcript_text = " ".join([
        seg["text"] + (" [í™”ë©´: " + " ".join(seg.get("screen_text", [])) + "]" if seg.get("screen_text") else "")
        for seg in segments
    ])

    corrections = extract_corrections_from_transcript(transcript_text)
    corrected_segments = apply_corrections_to_segments(segments, corrections)
    chunked = chunk_segments_with_llm(corrected_segments, video_filename)
    docs = build_documents_from_chunks(chunked, video_url, video_filename)
    
    all_docs.extend(docs)  # ëˆ„ì 
    print(f"âœ… {video_filename}: {len(docs)}ê°œ docs")

# ì „ë¶€ ì²˜ë¦¬ í›„ ì €ì¥
print(f"\nì´ {len(all_docs)}ê°œ documents")
embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
vectorstore = Chroma.from_documents(
    documents=all_docs,
    embedding=embeddings,
    persist_directory="./chroma_db_with_role"
)
print("âœ… ì €ì¥ ì™„ë£Œ!")
