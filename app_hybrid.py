import json
import unicodedata
import streamlit as st
from dotenv import load_dotenv
from rapidfuzz import process, fuzz

# LangChain (LLM + VectorStore)
from langchain_openai import ChatOpenAI
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# ğŸ”¥ ê³ ê¸‰ retrievers

from langchain_community.retrievers.contextual_compression import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import LLMChainExtractor  # compression
from langchain_community.retrievers.contextual_compression import (
    ContextualCompressionRetriever
)

# LCEL + Message History
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory


# ============================================================
# 1. UI / APP ì´ˆê¸°í™”
# ============================================================

def init_page():
    st.set_page_config(page_title="ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸", page_icon="ğŸ“š")
    st.title("ğŸ“š ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸")
    st.markdown("ì˜ìƒ ê¸°ë°˜ ë§ì¶¤í˜• í•™ìŠµ ì‹œìŠ¤í…œ")


def init_env():
    load_dotenv()


AVAILABLE_TOPICS = """
í˜„ì¬ í•™ìŠµ ê°€ëŠ¥í•œ ì£¼ì œ:
1. ì‹œê°„ ì „ì¹˜ì‚¬ (at, on, in)
2. ì¥ì†Œ ì „ì¹˜ì‚¬ (at, on, in)
3. ì†Œìœ ê²© (my, mine, his)
4. Does he? vs Is he?
5. ìˆ˜ë™íƒœ (be + ê³¼ê±°ë¶„ì‚¬)
6. í˜„ì¬ì™„ë£Œ
7. that ìš©ë²•
8. I'm not used to íŒ¨í„´
9. Do you? vs Are you?
"""


# ============================================================
# 2. ë°ì´í„° ë¡œë”©
# ============================================================

@st.cache_resource
def load_vectorstore():
    with st.spinner("ë²¡í„°ìŠ¤í† ì–´ ë¡œë”© ì¤‘..."):
        embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
        vectorstore = Chroma(
            persist_directory="./chroma_db",
            embedding_function=embeddings,
            collection_name="english_grammar_chunked",
        )
    return vectorstore


@st.cache_resource
def load_knowledge_graph():
    with open(
        "/data/edutem/sooine/rag_bot/knowledge_graph.json",
        "r",
        encoding="utf-8",
    ) as f:
        return json.load(f)


# ============================================================
# 3. ì§€ì‹ ê·¸ë˜í”„ ê²€ìƒ‰ ìœ í‹¸
# ============================================================

def normalize(s: str):
    return unicodedata.normalize("NFC", s.lower().replace(" ", ""))


def fuzzy_match_topic(query, topic_list):
    q = normalize(query)
    candidates = [normalize(t) for t in topic_list]
    match, score, idx = process.extractOne(q, candidates, scorer=fuzz.ratio)
    return topic_list[idx] if score > 70 else None


def search_in_knowledge_graph(query, knowledge_graph):
    q = query.lower().strip()
    topic_list = list(knowledge_graph.keys())

    best = fuzzy_match_topic(query, topic_list)
    if best:
        return {"type": "main_topic", "main_topic": best, "data": knowledge_graph[best]}

    # ì •í™• ì¼ì¹˜
    for topic in topic_list:
        if topic.lower() == q:
            return {"type": "main_topic", "main_topic": topic, "data": knowledge_graph[topic]}

    # ë¶€ë¶„ ì¼ì¹˜
    for topic in topic_list:
        if q in topic.lower():
            return {"type": "main_topic", "main_topic": topic, "data": knowledge_graph[topic]}

    # Sub-topic ê²€ìƒ‰
    best_match = None
    best_score = 0

    for main_topic, topic_data in knowledge_graph.items():
        for sub_id, sub_data in topic_data["sub_topics"].items():
            score = 0
            if q in sub_data["title"].lower(): score += 3
            if q in sub_data["concept"].lower(): score += 2
            for ex in sub_data.get("examples", []):
                if q in ex.lower():
                    score += 1
                    break
            if score > best_score:
                best_score = score
                best_match = {
                    "type": "sub_topic",
                    "main_topic": main_topic,
                    "sub_topic_id": sub_id,
                    "data": sub_data,
                }

    if best_match and best_score >= 1:
        return best_match

    return None


# ============================================================
# 4. ì„¸ì…˜ & íˆìŠ¤í† ë¦¬
# ============================================================

def init_session_state():
    if "store" not in st.session_state:
        st.session_state["store"] = {}
    if "session_id" not in st.session_state:
        st.session_state["session_id"] = "default_user"
    if "mode" not in st.session_state:
        st.session_state["mode"] = "search"


def get_session_history(session_id):
    store = st.session_state["store"]
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


# ============================================================
# 5. LLM / Retriever / RAG êµ¬ì„±
# ============================================================

def init_llm():
    return ChatOpenAI(model="gpt-4o", temperature=0)


def build_prompt():
    return ChatPromptTemplate.from_messages([
        ("system", "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ì˜ì–´ í•™ìŠµ ë„ìš°ë¯¸ì…ë‹ˆë‹¤. ì•„ë˜ contextë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.\n\n<context>\n{context}\n</context>"),
        MessagesPlaceholder("chat_history"),
        ("human", "{question}")
    ])


# ğŸ”¥ í•µì‹¬ â€” MultiQuery â†’ Ensemble â†’ Compression

def build_advanced_retriever(vectorstore, llm_for_retrieval):
    # 1) Base retriever (ë²¡í„° ê²€ìƒ‰)
    base_retriever = vectorstore.as_retriever(search_kwargs={"k": 15})

    # 2) LLM ê¸°ë°˜ ë¬¸ì„œ ì••ì¶•ê¸°
    compressor = LLMChainExtractor.from_llm(llm_for_retrieval)

    # 3) ì••ì¶• retriever = base_retriever + compressor
    compression_retriever = ContextualCompressionRetriever(
        base_retriever=base_retriever,
        base_compressor=compressor
    )

    return compression_retriever


def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


def build_rag_chain(retriever, llm, prompt):
    def rag_with_chain(inputs):
        q = inputs["question"]
        hist = inputs.get("chat_history", [])

        docs = retriever.invoke(q)

        answer = (prompt | llm | StrOutputParser()).invoke({
            "context": format_docs(docs),
            "question": q,
            "chat_history": hist,
        })

        return {"answer": answer, "source_docs": docs}

    rag_chain = RunnableLambda(rag_with_chain)

    return RunnableWithMessageHistory(
        rag_chain,
        get_session_history,
        input_messages_key="question",
        history_messages_key="chat_history"
    )


# ============================================================
# 6. UI
# ============================================================

def render_sidebar():
    with st.sidebar:
        st.header("âš™ï¸ ì„¤ì •")
        mode = st.radio("ëª¨ë“œ ì„ íƒ", ["ğŸ” Search", "ğŸ“ Quiz", "ğŸ“– Review"], index=0)
        st.session_state["mode"] = mode
        st.divider()
        st.markdown("### ğŸ“š í•™ìŠµ ì£¼ì œ")
        st.markdown(AVAILABLE_TOPICS)


def render_chat(chain, knowledge_graph):
    session_id = st.session_state["session_id"]
    history = get_session_history(session_id)

    # ê¸°ì¡´ ëŒ€í™” ì¶œë ¥
    for msg in reversed(history.messages):
        role = "user" if msg.type == "human" else "assistant"
        with st.chat_message(role):
            st.markdown(msg.content)

    user_input = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”...")
    if not user_input:
        return

    with st.chat_message("user"):
        st.markdown(user_input)

    # -------------------------------
    # ğŸ”¥ 1) ì§€ì‹ ê·¸ë˜í”„ ìš°ì„  ê²€ìƒ‰
    # -------------------------------
    kg_result = search_in_knowledge_graph(user_input, knowledge_graph)

    if kg_result:
        with st.chat_message("assistant"):
            st.markdown("### ğŸ“˜ ì§€ì‹ ê·¸ë˜í”„ ì„¤ëª…")

            if kg_result["type"] == "main_topic":
                st.write(kg_result["data"]["concept"])
                st.markdown("#### ì˜ˆë¬¸")
                for ex in kg_result["data"].get("examples", []):
                    st.write(f"- {ex}")
            else:
                st.write(kg_result["data"]["concept"])
                st.markdown("#### ì˜ˆë¬¸")
                for ex in kg_result["data"].get("examples", []):
                    st.write(f"- {ex}")
        return

    # -------------------------------
    # ğŸ”¥ 2) ì§€ì‹ ê·¸ë˜í”„ì— ì—†ìœ¼ë©´ â†’ RAG
    # -------------------------------
    with st.chat_message("assistant"):
        result = chain.invoke(
            {"question": user_input},
            config={"configurable": {"session_id": session_id}},
        )
        st.markdown("### ğŸ’¡ ì„¤ëª…")
        st.write(result["answer"])



# ============================================================
# 7. main()
# ============================================================

def main():
    init_page()
    init_env()
    init_session_state()

    vectorstore = load_vectorstore()
    knowledge_graph = load_knowledge_graph()

    llm = init_llm()
    prompt = build_prompt()
    retriever = build_advanced_retriever(vectorstore, llm)
    chain = build_rag_chain(retriever, llm, prompt)

    render_sidebar()
    render_chat(chain, knowledge_graph)


if __name__ == "__main__":
    main()
