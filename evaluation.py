"""
RAG í‰ê°€ ìŠ¤í¬ë¦½íŠ¸
- Retrieval í’ˆì§ˆ ì¸¡ì •
- Answer í’ˆì§ˆ ì¸¡ì • (RAGAS)
"""
import json
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain_openai import ChatOpenAI

load_dotenv()

# =============================================================================
# 1. í…ŒìŠ¤íŠ¸ì…‹ ì •ì˜
# =============================================================================

TEST_SET = [
    {
        "question": "beë™ì‚¬ê°€ ë­ì•¼?",
        "expected_keywords": ["beë™ì‚¬", "am", "is", "are"],
        "expected_topic": "beë™ì‚¬",
    },
    {
        "question": "at, on, in ì°¨ì´ê°€ ë­ì•¼?",
        "expected_keywords": ["at", "on", "in", "ì „ì¹˜ì‚¬", "ì‹œê°„", "ì¥ì†Œ"],
        "expected_topic": "ì „ì¹˜ì‚¬",
    },
    {
        "question": "I am busy ë§ëŠ” ë¬¸ì¥ì´ì•¼?",
        "expected_keywords": ["am", "busy", "í˜•ìš©ì‚¬"],
        "expected_topic": "beë™ì‚¬",
    },
    {
        "question": "í˜„ì¬ì™„ë£Œ ì–¸ì œ ì¨?",
        "expected_keywords": ["have", "has", "ê³¼ê±°ë¶„ì‚¬", "í˜„ì¬ì™„ë£Œ"],
        "expected_topic": "í˜„ì¬ì™„ë£Œ",
    },
    {
        "question": "ìˆ˜ë™íƒœ ë§Œë“œëŠ” ë²•",
        "expected_keywords": ["be", "ê³¼ê±°ë¶„ì‚¬", "ìˆ˜ë™íƒœ"],
        "expected_topic": "ìˆ˜ë™íƒœ",
    },
]


# =============================================================================
# 2. Vectorstore ë¡œë“œ
# =============================================================================

def load_vectorstore():
    embeddings = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-large")
    vectorstore = Chroma(
        persist_directory="./chroma_db",
        embedding_function=embeddings,
        collection_name="english_grammar_chunked",
    )
    return vectorstore


# =============================================================================
# 3. ì œì™¸í•  topic ëª©ë¡
# =============================================================================

EXCLUDE_TOPICS = [
    "ê°•ì˜ ë§ˆë¬´ë¦¬", "ê°•ì˜ ì¢…ë£Œ", "ìˆ˜ì—… ì¢…ë£Œ", "ì¸ì‚¬",
    "ê°•ì˜ ì‹œì‘", "ìˆ˜ì—… ì‹œì‘", "ë§ˆë¬´ë¦¬"
]


# =============================================================================
# 4. Retrieval í‰ê°€
# =============================================================================

def evaluate_retrieval(vectorstore, test_set, k=5, use_filter=True):
    """
    ê° ì§ˆë¬¸ì— ëŒ€í•´ ê²€ìƒ‰ ê²°ê³¼ í‰ê°€
    - Hit Rate: ê´€ë ¨ í‚¤ì›Œë“œê°€ top-kì— í¬í•¨ë˜ëŠ”ì§€
    - MRR: ì²« ë²ˆì§¸ ê´€ë ¨ ê²°ê³¼ì˜ ìˆœìœ„
    """
    results = []

    for item in test_set:
        query = item["question"]
        expected_keywords = item["expected_keywords"]

        # ê²€ìƒ‰ ì‹¤í–‰ (í•„í„° ì ìš© ì—¬ë¶€)
        if use_filter:
            docs_with_scores = vectorstore.similarity_search_with_score(
                query,
                k=k,
                filter={"topic": {"$nin": EXCLUDE_TOPICS}}
            )
        else:
            docs_with_scores = vectorstore.similarity_search_with_score(query, k=k)

        # ê²°ê³¼ ë¶„ì„
        hit = False
        first_hit_rank = None
        retrieved_texts = []

        for rank, (doc, score) in enumerate(docs_with_scores, 1):
            text = doc.page_content.lower()
            retrieved_texts.append({
                "rank": rank,
                "score": float(score),
                "text": doc.page_content[:200],
                "metadata": doc.metadata,
            })

            # í‚¤ì›Œë“œ ë§¤ì¹­ í™•ì¸
            for kw in expected_keywords:
                if kw.lower() in text:
                    hit = True
                    if first_hit_rank is None:
                        first_hit_rank = rank
                    break

        results.append({
            "question": query,
            "expected_topic": item["expected_topic"],
            "hit": hit,
            "first_hit_rank": first_hit_rank,
            "mrr": 1/first_hit_rank if first_hit_rank else 0,
            "retrieved": retrieved_texts,
        })

    return results


def print_retrieval_report(results):
    """í‰ê°€ ê²°ê³¼ ì¶œë ¥"""
    print("\n" + "="*80)
    print("RETRIEVAL í‰ê°€ ê²°ê³¼")
    print("="*80)

    total_hits = sum(1 for r in results if r["hit"])
    avg_mrr = sum(r["mrr"] for r in results) / len(results)

    print(f"\nğŸ“Š ì „ì²´ ì§€í‘œ:")
    print(f"   Hit Rate @ 5: {total_hits}/{len(results)} ({total_hits/len(results)*100:.1f}%)")
    print(f"   í‰ê·  MRR: {avg_mrr:.3f}")

    print(f"\nğŸ“‹ ê°œë³„ ê²°ê³¼:")
    for r in results:
        status = "âœ…" if r["hit"] else "âŒ"
        print(f"\n{status} ì§ˆë¬¸: {r['question']}")
        print(f"   ê¸°ëŒ€ ì£¼ì œ: {r['expected_topic']}")
        print(f"   Hit: {r['hit']}, First Hit Rank: {r['first_hit_rank']}")
        print(f"   Top-3 ê²€ìƒ‰ ê²°ê³¼:")
        for doc in r["retrieved"][:3]:
            print(f"      [{doc['rank']}] (score: {doc['score']:.3f}) {doc['text'][:80]}...")


# =============================================================================
# 4. LLM-as-Judge í‰ê°€ (ì„ íƒì )
# =============================================================================

def evaluate_with_llm_judge(question, context, answer):
    """LLMìœ¼ë¡œ ë‹µë³€ í’ˆì§ˆ í‰ê°€"""
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

    prompt = f"""ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì˜ í’ˆì§ˆì„ í‰ê°€í•´ì£¼ì„¸ìš”.

ì§ˆë¬¸: {question}

ì œê³µëœ ì»¨í…ìŠ¤íŠ¸:
{context}

ìƒì„±ëœ ë‹µë³€:
{answer}

í‰ê°€ ê¸°ì¤€:
1. ê´€ë ¨ì„± (1-5): ë‹µë³€ì´ ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ”ê°€?
2. ì •í™•ì„± (1-5): ì»¨í…ìŠ¤íŠ¸ì— ê¸°ë°˜í•œ ì •í™•í•œ ì •ë³´ì¸ê°€?
3. ì™„ì „ì„± (1-5): ì§ˆë¬¸ì— ì¶©ë¶„íˆ ë‹µë³€í–ˆëŠ”ê°€?

JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€í•´ì£¼ì„¸ìš”:
{{"relevance": X, "accuracy": X, "completeness": X, "comment": "..."}}
"""

    response = llm.invoke(prompt)
    try:
        return json.loads(response.content)
    except:
        return {"error": response.content}


# =============================================================================
# 5. ë©”ì¸ ì‹¤í–‰
# =============================================================================

if __name__ == "__main__":
    print("ğŸ”„ Vectorstore ë¡œë”©...")
    vectorstore = load_vectorstore()

    # í•„í„° ì—†ì´ í‰ê°€
    print("\n" + "="*80)
    print("ğŸ“Œ í•„í„° ì—†ì´ í‰ê°€ (ì›ë³¸)")
    print("="*80)
    results_no_filter = evaluate_retrieval(vectorstore, TEST_SET, k=5, use_filter=False)
    print_retrieval_report(results_no_filter)

    # í•„í„° ì ìš© í‰ê°€
    print("\n" + "="*80)
    print("ğŸ“Œ í•„í„° ì ìš© í‰ê°€ (ê°•ì˜ ë§ˆë¬´ë¦¬ ë“± ì œì™¸)")
    print("="*80)
    results_filtered = evaluate_retrieval(vectorstore, TEST_SET, k=5, use_filter=True)
    print_retrieval_report(results_filtered)

    # ê²°ê³¼ ë¹„êµ
    hits_before = sum(1 for r in results_no_filter if r["hit"])
    hits_after = sum(1 for r in results_filtered if r["hit"])
    mrr_before = sum(r["mrr"] for r in results_no_filter) / len(results_no_filter)
    mrr_after = sum(r["mrr"] for r in results_filtered) / len(results_filtered)

    print("\n" + "="*80)
    print("ğŸ“Š í•„í„° ì ìš© ì „í›„ ë¹„êµ")
    print("="*80)
    print(f"Hit Rate: {hits_before}/5 â†’ {hits_after}/5")
    print(f"MRR: {mrr_before:.3f} â†’ {mrr_after:.3f}")

    # ê²°ê³¼ ì €ì¥
    with open("evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump({"no_filter": results_no_filter, "filtered": results_filtered}, f, ensure_ascii=False, indent=2)
    print("\nğŸ’¾ ê²°ê³¼ê°€ evaluation_results.jsonì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
